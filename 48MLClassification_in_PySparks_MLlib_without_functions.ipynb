{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaledn66/pyspark2/blob/main/48MLClassification_in_PySparks_MLlib_without_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKyQb7lzKuBI"
      },
      "source": [
        "# Classification in PySpark's MLlib\n",
        "\n",
        "PySpark offers a good variety of algorithms that can be applied to classification machine learning problems. However, because PySpark operates on distributed dataframes, we cannot use popular Python libraries like scikit learn for our machine learning applications. Which means we need to use PySpark's MLlib packages for these tasks. Luckily, MLlib offers a pretty good variety of algorithms! In this notebook we will go over how to prep our data and train and test the classification algorithms PySpark offers.\n",
        "\n",
        "## Defining Classification\n",
        "\n",
        "As we went over in the concept review lecture, classification is a supervised machine learning task where we want to automatically categorize our data into some pre-defined categorization method. Examples of classification might include sorting objects like flowers into various species or automatically labeling images into groups like cat, dog, fish, etc. To be able to do this though, we need to have training data and a pre-defined dependent variable which is the column in your dataset that defines the categories you want to predict.\n",
        "\n",
        "## Algorithms Available\n",
        "\n",
        "PySpark offers the following algorithms for classification.\n",
        "\n",
        "1. Logistic Regression\n",
        "2. Naive Bayes\n",
        "3. One Vs Rest\n",
        "4. Linear Support Vector Machine (SVC)\n",
        "5. Random Forest Classifier\n",
        "6. GBT Classifier\n",
        "7. Decision Tree Classifier\n",
        "8. Multilayer Perceptron Classifier (Neural Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "xobiP1cdKuBM",
        "outputId": "e75a2b4c-f405-4606-b24b-d6006695b9d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are working with 1 core(s)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78cda9303a60>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://246a02f202c0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Classification</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# First let's create our PySpark instance\n",
        "# import findspark\n",
        "# findspark.init()\n",
        "\n",
        "import pyspark # only run after findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "# May take awhile locally\n",
        "spark = SparkSession.builder.appName(\"Classification\").getOrCreate()\n",
        "\n",
        "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
        "print(\"You are working with\", cores, \"core(s)\")\n",
        "spark\n",
        "# Click the hyperlinked \"Spark UI\" link to view details about your Spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kqHNnUw7KuBO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Read in functions we will need\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZBdq4o9KuBO"
      },
      "source": [
        "## Let's read our dataset in for this notebook\n",
        "\n",
        "### Data Set Name: Autistic Spectrum Disorder Screening Data for Adult\n",
        "Autistic Spectrum Disorder (ASD) is a neurodevelopment condition associated with significant healthcare costs, and early diagnosis can significantly reduce these. Unfortunately, waiting times for an ASD diagnosis are lengthy and procedures are not cost effective. The economic impact of autism and the increase in the number of ASD cases across the world reveals an urgent need for the development of easily implemented and effective screening methods. Therefore, a time-efficient and accessible ASD screening is imminent to help health professionals and inform individuals whether they should pursue formal clinical diagnosis. The rapid growth in the number of ASD cases worldwide necessitates datasets related to behaviour traits. However, such datasets are rare making it difficult to perform thorough analyses to improve the efficiency, sensitivity, specificity and predictive accuracy of the ASD screening process. Presently, very limited autism datasets associated with clinical or screening are available and most of them are genetic in nature. Hence, we propose a new dataset related to autism screening of adults that contained 20 features to be utilised for further analysis especially in determining influential autistic traits and improving the classification of ASD cases. In this dataset, we record ten behavioural features (AQ-10-Adult) plus ten individuals characteristics that have proved to be effective in detecting the ASD cases from controls in behaviour science.\n",
        "\n",
        "### Source:\n",
        "https://www.kaggle.com/faizunnabi/autism-screening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MgkVRjjmKuBO"
      },
      "outputs": [],
      "source": [
        "path =\"./\"\n",
        "df = spark.read.csv(path+'Toddler Autism dataset July 2018.csv',inferSchema=True,header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkNMPM4mKuBP"
      },
      "source": [
        "### Check out the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "UMPdBzrAKuBP",
        "outputId": "e63d0ab1-ab05-4dfe-c6d0-398c9e56053a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Case_No  A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  Age_Mons  Qchat-10-Score  \\\n",
              "0        1   0   0   0   0   0   0   1   1   0    1        28               3   \n",
              "1        2   1   1   0   0   0   1   1   0   0    0        36               4   \n",
              "2        3   1   0   0   0   0   0   1   1   0    1        36               4   \n",
              "3        4   1   1   1   1   1   1   1   1   1    1        24              10   \n",
              "4        5   1   1   0   1   1   1   1   1   1    1        20               9   \n",
              "5        6   1   1   0   0   1   1   1   1   1    1        21               8   \n",
              "\n",
              "  Sex       Ethnicity Jaundice Family_mem_with_ASD Who completed the test  \\\n",
              "0   f  middle eastern      yes                  no          family member   \n",
              "1   m  White European      yes                  no          family member   \n",
              "2   m  middle eastern      yes                  no          family member   \n",
              "3   m        Hispanic       no                  no          family member   \n",
              "4   f  White European       no                 yes          family member   \n",
              "5   m           black       no                  no          family member   \n",
              "\n",
              "  Class/ASD Traits   \n",
              "0                No  \n",
              "1               Yes  \n",
              "2               Yes  \n",
              "3               Yes  \n",
              "4               Yes  \n",
              "5               Yes  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d67a8b43-cebd-4af3-9a73-4586cdc6b9f4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Case_No</th>\n",
              "      <th>A1</th>\n",
              "      <th>A2</th>\n",
              "      <th>A3</th>\n",
              "      <th>A4</th>\n",
              "      <th>A5</th>\n",
              "      <th>A6</th>\n",
              "      <th>A7</th>\n",
              "      <th>A8</th>\n",
              "      <th>A9</th>\n",
              "      <th>A10</th>\n",
              "      <th>Age_Mons</th>\n",
              "      <th>Qchat-10-Score</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>Jaundice</th>\n",
              "      <th>Family_mem_with_ASD</th>\n",
              "      <th>Who completed the test</th>\n",
              "      <th>Class/ASD Traits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>3</td>\n",
              "      <td>f</td>\n",
              "      <td>middle eastern</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>family member</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "      <td>4</td>\n",
              "      <td>m</td>\n",
              "      <td>White European</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>family member</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>4</td>\n",
              "      <td>m</td>\n",
              "      <td>middle eastern</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>family member</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>10</td>\n",
              "      <td>m</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>family member</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>9</td>\n",
              "      <td>f</td>\n",
              "      <td>White European</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>family member</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>8</td>\n",
              "      <td>m</td>\n",
              "      <td>black</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>family member</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d67a8b43-cebd-4af3-9a73-4586cdc6b9f4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d67a8b43-cebd-4af3-9a73-4586cdc6b9f4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d67a8b43-cebd-4af3-9a73-4586cdc6b9f4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cfc96d9c-5a45-448d-aa1f-432d5eceb2f2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cfc96d9c-5a45-448d-aa1f-432d5eceb2f2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cfc96d9c-5a45-448d-aa1f-432d5eceb2f2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"Case_No\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A1\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A2\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A3\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A4\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A5\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A6\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A7\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A8\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A9\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A10\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age_Mons\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          36\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Qchat-10-Score\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"m\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ethnicity\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"White European\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Jaundice\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"no\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Family_mem_with_ASD\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"yes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Who completed the test\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"family member\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Class/ASD Traits \",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Yes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "df.limit(6).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "G4fEWQiIKuBP",
        "outputId": "048afc15-7120-4bd2-e09c-a83c1863a4fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Case_No: integer (nullable = true)\n",
            " |-- A1: integer (nullable = true)\n",
            " |-- A2: integer (nullable = true)\n",
            " |-- A3: integer (nullable = true)\n",
            " |-- A4: integer (nullable = true)\n",
            " |-- A5: integer (nullable = true)\n",
            " |-- A6: integer (nullable = true)\n",
            " |-- A7: integer (nullable = true)\n",
            " |-- A8: integer (nullable = true)\n",
            " |-- A9: integer (nullable = true)\n",
            " |-- A10: integer (nullable = true)\n",
            " |-- Age_Mons: integer (nullable = true)\n",
            " |-- Qchat-10-Score: integer (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Ethnicity: string (nullable = true)\n",
            " |-- Jaundice: string (nullable = true)\n",
            " |-- Family_mem_with_ASD: string (nullable = true)\n",
            " |-- Who completed the test: string (nullable = true)\n",
            " |-- Class/ASD Traits : string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_dJfEHOKuBP"
      },
      "source": [
        "### How many classes do we have?\n",
        "\n",
        "It's important to check for class imbalance in your dependent variable for classification tasks. If there are extremley under or over represented classes, the accuracy of your model predictions might suffer as a result of your model essentially being biased.\n",
        "\n",
        "If you see class imbalance, one common way to correct this would be boot strapping or resampling your dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB9kYMOjKuBQ",
        "outputId": "142f3329-2a59-47c2-ec9f-77135a812a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|Class/ASD Traits |count|\n",
            "+-----------------+-----+\n",
            "|               No|  326|\n",
            "|              Yes|  728|\n",
            "+-----------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy(\"Class/ASD Traits \").count().show(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYho_WUdKuBQ"
      },
      "source": [
        "## Format Data\n",
        "\n",
        "MLlib requires all input columns of your dataframe to be vectorized. You will see that we rename our dependent var to label as that is what is expected for all MLlib applications. If rename once here, we never have to do it again!\n",
        "\n",
        "For more methods on transformations visit: https://spark.apache.org/docs/latest/ml-features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "jCbsKQYyKuBQ"
      },
      "outputs": [],
      "source": [
        "# Declare values you will need\n",
        "\n",
        "# col_list = [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\",\"Age_Mons\",\"Qchat-10-Score\",\"Sex\",\"Ethnicity\",\"Jaundice\",\"Family_mem_with_ASD\",\"Who completed the test\"]\n",
        "# input_columns = col_list\n",
        "\n",
        "input_columns = df.columns # Collect the column names as a list\n",
        "input_columns = input_columns[1:-1] # keep only relevant columns: from column 1 to\n",
        "\n",
        "dependent_var = 'Class/ASD Traits '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b6u17jvKuBQ",
        "outputId": "8c551bc9-e19d-4905-f8f7-fa1a3c02c865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+---+---+---+---+---+---+---+---+---+--------+--------------+---+--------------+--------+-------------------+----------------------+-----------------+---------+-----+\n",
            "|Case_No| A1| A2| A3| A4| A5| A6| A7| A8| A9|A10|Age_Mons|Qchat-10-Score|Sex|     Ethnicity|Jaundice|Family_mem_with_ASD|Who completed the test|Class/ASD Traits |label_str|label|\n",
            "+-------+---+---+---+---+---+---+---+---+---+---+--------+--------------+---+--------------+--------+-------------------+----------------------+-----------------+---------+-----+\n",
            "|      1|  0|  0|  0|  0|  0|  0|  1|  1|  0|  1|      28|             3|  f|middle eastern|     yes|                 no|         family member|               No|       No|  1.0|\n",
            "|      2|  1|  1|  0|  0|  0|  1|  1|  0|  0|  0|      36|             4|  m|White European|     yes|                 no|         family member|              Yes|      Yes|  0.0|\n",
            "|      3|  1|  0|  0|  0|  0|  0|  1|  1|  0|  1|      36|             4|  m|middle eastern|     yes|                 no|         family member|              Yes|      Yes|  0.0|\n",
            "|      4|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|      24|            10|  m|      Hispanic|      no|                 no|         family member|              Yes|      Yes|  0.0|\n",
            "|      5|  1|  1|  0|  1|  1|  1|  1|  1|  1|  1|      20|             9|  f|White European|      no|                yes|         family member|              Yes|      Yes|  0.0|\n",
            "|      6|  1|  1|  0|  0|  1|  1|  1|  1|  1|  1|      21|             8|  m|         black|      no|                 no|         family member|              Yes|      Yes|  0.0|\n",
            "|      7|  1|  0|  0|  1|  1|  1|  0|  0|  1|  0|      33|             5|  m|         asian|     yes|                 no|         family member|              Yes|      Yes|  0.0|\n",
            "|      8|  0|  1|  0|  0|  1|  0|  1|  1|  1|  1|      33|             6|  m|         asian|     yes|                 no|         family member|              Yes|      Yes|  0.0|\n",
            "|      9|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|      36|             2|  m|         asian|      no|                 no|         family member|               No|       No|  1.0|\n",
            "|     10|  1|  1|  1|  0|  1|  1|  0|  1|  1|  1|      22|             8|  m|   south asian|      no|                 no|  Health Care Profe...|              Yes|      Yes|  0.0|\n",
            "|     11|  1|  0|  0|  1|  0|  1|  1|  0|  1|  1|      36|             6|  m|      Hispanic|     yes|                yes|         family member|              Yes|      Yes|  0.0|\n",
            "|     12|  1|  1|  1|  1|  0|  1|  1|  1|  0|  1|      17|             8|  m|middle eastern|     yes|                 no|         family member|              Yes|      Yes|  0.0|\n",
            "|     13|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      25|             0|  f|middle eastern|     yes|                 no|         family member|               No|       No|  1.0|\n",
            "|     14|  1|  1|  1|  1|  0|  0|  1|  0|  1|  1|      15|             7|  f|middle eastern|     yes|                 no|         family member|              Yes|      Yes|  0.0|\n",
            "|     15|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      18|             0|  m|middle eastern|      no|                 no|         family member|               No|       No|  1.0|\n",
            "|     16|  1|  1|  1|  0|  1|  0|  1|  1|  0|  1|      12|             7|  m|         black|      no|                 no|         family member|              Yes|      Yes|  0.0|\n",
            "|     17|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      36|             0|  m|middle eastern|      no|                yes|         family member|               No|       No|  1.0|\n",
            "|     18|  1|  1|  1|  0|  1|  1|  1|  1|  0|  1|      12|             8|  f|middle eastern|     yes|                 no|         family member|              Yes|      Yes|  0.0|\n",
            "|     19|  1|  0|  0|  0|  1|  0|  0|  0|  0|  1|      29|             3|  f|middle eastern|      no|                 no|         family member|               No|       No|  1.0|\n",
            "|     20|  1|  1|  1|  0|  1|  0|  1|  1|  0|  1|      12|             7|  f|         black|      no|                 no|         family member|              Yes|      Yes|  0.0|\n",
            "+-------+---+---+---+---+---+---+---+---+---+---+--------+--------------+---+--------------+--------+-------------------+----------------------+-----------------+---------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# change label (class variable) to string type to prep for reindexing\n",
        "# Pyspark is expecting a zero indexed integer for the label column.\n",
        "# Just in case our data is not in that format... we will treat it by using the StringIndexer built in method\n",
        "renamed = df.withColumn(\"label_str\", df[dependent_var].cast(StringType())) #Rename and change to string type\n",
        "indexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\") #Pyspark is expecting the this naming convention\n",
        "indexed = indexer.fit(renamed).transform(renamed)\n",
        "indexed.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "zd7W0V1pKuBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "253792c2-2027-46a9-85e8-00ee2a270b88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A1',\n",
              " 'A2',\n",
              " 'A3',\n",
              " 'A4',\n",
              " 'A5',\n",
              " 'A6',\n",
              " 'A7',\n",
              " 'A8',\n",
              " 'A9',\n",
              " 'A10',\n",
              " 'Age_Mons',\n",
              " 'Qchat-10-Score',\n",
              " 'Sex',\n",
              " 'Ethnicity',\n",
              " 'Jaundice',\n",
              " 'Family_mem_with_ASD',\n",
              " 'Who completed the test']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Convert all string type data in the input column list to numeric\n",
        "# Otherwise the Algorithm will not be able to process it\n",
        "\n",
        "# Also we will use these lists later on\n",
        "numeric_inputs = []\n",
        "string_inputs = []\n",
        "for column in input_columns:\n",
        "    # First identify the string vars in your input column list\n",
        "    if str(indexed.schema[column].dataType) == 'StringType':\n",
        "        # Set up your String Indexer function\n",
        "        indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\")\n",
        "        # Then call on the indexer you created here\n",
        "        indexed = indexer.fit(indexed).transform(indexed)\n",
        "        # Rename the column to a new name so you can disinguish it from the original\n",
        "        new_col_name = column+\"_num\"\n",
        "        # Add the new column name to the string inputs list\n",
        "        string_inputs.append(new_col_name)\n",
        "    else:\n",
        "        # If no change was needed, take no action\n",
        "        # And add the numeric var to the num list\n",
        "        numeric_inputs.append(column)\n",
        "numeric_inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**from  Chat GPT **"
      ],
      "metadata": {
        "id": "aUEnL6MAmdLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Sample DataFrame\n",
        "data = spark.createDataFrame([\n",
        "    (0, \"cat\"),\n",
        "    (1, \"dog\"),\n",
        "    (2, \"mouse\"),\n",
        "    (3, \"cat\"),\n",
        "    (4, \"dog\"),\n",
        "    (5, \"cat\")\n",
        "], [\"id\", \"animal\"])\n",
        "\n",
        "# Apply StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"animal\", outputCol=\"animal_index\")\n",
        "indexed = indexer.fit(data).transform(data)\n",
        "\n",
        "indexed.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmHhzMm-iIGv",
        "outputId": "0daac143-067e-4540-b270-117ffeb6f342"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------------+\n",
            "| id|animal|animal_index|\n",
            "+---+------+------------+\n",
            "|  0|   cat|         0.0|\n",
            "|  1|   dog|         1.0|\n",
            "|  2| mouse|         2.0|\n",
            "|  3|   cat|         0.0|\n",
            "|  4|   dog|         1.0|\n",
            "|  5|   cat|         0.0|\n",
            "+---+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpmoL_QKKuBR"
      },
      "source": [
        "#### Treating for skewness and outliers\n",
        "\n",
        "Recall from the Model Selection and Tuning lecture that Skewness measures how much a distribution of values deviates from symmetry around the mean. A value of zero means the distribution is symmetric, while a positive skewness indicates a greater number of smaller values, and a negative value indicates a greater number of larger values.\n",
        "\n",
        "As a general rule of thumb:\n",
        "\n",
        " - If skewness is **less than -1 or greater than 1**, the distribution is highly skewed.\n",
        " - If skewness is **between -1 and -0.5 or between 0.5 and 1**, the distribution is moderately skewed.\n",
        " - If skewness is **between -0.5 and 0.5**, the distribution is approximately symmetric.\n",
        "\n",
        "A common recommendation for treating skewness is either a log transformation for positive skewed data or an exponential transformation for negatively skewed data.\n",
        "\n",
        "\n",
        "**Outliers** <br>\n",
        "One common way to correct outliers is by flooring and capping which means editing any value that is above or below a certain threshold (99th percentile or 1st percentile) back to the highest/lowest value in that percentile. For example, if the 99th percentile is 96 and there is a value of 1,000, you would change that value to 96."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Mh1cMQseKuBR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f8bd59c8-1ec3-4a7c-b2f4-b1aeec1a770a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: Quantile calculation for column Sex with data type StringType is not supported.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-111cf11efb5e>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# I'm doing the top and bottom 1% but you can adjust if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumeric_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#if you want to make it go faster increase the last number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#Now check for skewness for all numeric cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mapproxQuantile\u001b[0;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[1;32m   4845\u001b[0m         \u001b[0mrelativeError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4847\u001b[0;31m         \u001b[0mjaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4848\u001b[0m         \u001b[0mjaq_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjaq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4849\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misStr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Quantile calculation for column Sex with data type StringType is not supported."
          ]
        }
      ],
      "source": [
        "# Treat for skewness\n",
        "# Flooring and capping\n",
        "# Plus if right skew take the log +1\n",
        "# if left skew do exp transformation\n",
        "# This is best practice\n",
        "\n",
        "# create empty dictionary d\n",
        "d = {}\n",
        "# Create a dictionary of quantiles from your numeric cols\n",
        "# I'm doing the top and bottom 1% but you can adjust if needed\n",
        "for col in numeric_inputs:\n",
        "    d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) #if you want to make it go faster increase the last number\n",
        "\n",
        "#Now check for skewness for all numeric cols\n",
        "for col in numeric_inputs:\n",
        "    skew = indexed.agg(skewness(indexed[col])).collect() #check for skewness\n",
        "    skew = skew[0][0]\n",
        "    # If skewness is found,\n",
        "    # This function will make the appropriate corrections\n",
        "    if skew > 1: # If right skew, floor, cap and log(x+1)\n",
        "        indexed = indexed.withColumn(col, \\\n",
        "        log(when(df[col] < d[col][0],d[col][0])\\\n",
        "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
        "        .otherwise(indexed[col] ) +1).alias(col))\n",
        "        print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
        "    elif skew < -1: # If left skew floor, cap and exp(x)\n",
        "        indexed = indexed.withColumn(col, \\\n",
        "        exp(when(df[col] < d[col][0],d[col][0])\\\n",
        "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
        "        .otherwise(indexed[col] )).alias(col))\n",
        "        print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frm chat GPT"
      ],
      "metadata": {
        "id": "OBKmnB6eKcmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty dictionary for storing quantiles\n",
        "d = {}\n",
        "\n",
        "# Filter out non-numeric columns (ensure only numeric columns are considered)\n",
        "numeric_inputs = [col for col, dtype in indexed.dtypes if dtype in ['int', 'double', 'float']]\n",
        "\n",
        "# Create a dictionary of quantiles from your numeric columns\n",
        "for col in numeric_inputs:\n",
        "    d[col] = indexed.approxQuantile(col, [0.01, 0.99], 0.25)  # Adjust the last number for speed/accuracy\n",
        "\n",
        "# Now check for skewness in all numeric columns\n",
        "for col in numeric_inputs:\n",
        "    # Calculate skewness for the column\n",
        "    skew = indexed.agg(skewness(indexed[col])).collect()  # Get skewness value\n",
        "    skew = skew[0][0]\n",
        "\n",
        "    # Apply transformations based on the skewness value\n",
        "    if skew > 1:  # Right skew (positive skew)\n",
        "        indexed = indexed.withColumn(\n",
        "            col,\n",
        "            log(when(indexed[col] < d[col][0], d[col][0])  # Apply flooring\n",
        "                .when(indexed[col] > d[col][1], d[col][1])  # Apply capping\n",
        "                .otherwise(indexed[col]) + 1)  # Apply log(x + 1) for positive skew\n",
        "            .alias(col)\n",
        "        )\n",
        "        print(f\"{col} has been treated for positive (right) skewness. (skew = {skew})\")\n",
        "\n",
        "    elif skew < -1:  # Left skew (negative skew)\n",
        "        indexed = indexed.withColumn(\n",
        "            col,\n",
        "            exp(when(indexed[col] < d[col][0], d[col][0])  # Apply flooring\n",
        "                .when(indexed[col] > d[col][1], d[col][1])  # Apply capping\n",
        "                .otherwise(indexed[col]))  # Apply exp(x) for negative skew\n",
        "            .alias(col)\n",
        "        )\n",
        "        print(f\"{col} has been treated for negative (left) skewness. (skew = {skew})\")\n"
      ],
      "metadata": {
        "id": "u00rsbbaJ3aN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From chat GPT 2** improvement  шрифтом**"
      ],
      "metadata": {
        "id": "0Sc0niLXKlvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty dictionary for storing quantiles\n",
        "d = {}\n",
        "\n",
        "# Filter out non-numeric columns (ensure only numeric columns are considered)\n",
        "numeric_inputs = [col for col, dtype in indexed.dtypes if dtype in ['int', 'double', 'float']]\n",
        "\n",
        "# Debug: Print numeric column names\n",
        "print(\"Numeric columns to process:\", numeric_inputs)\n",
        "\n",
        "# Create a dictionary of quantiles from your numeric columns\n",
        "for col in numeric_inputs:\n",
        "    d[col] = indexed.approxQuantile(col, [0.01, 0.99], 0.25)  # Adjust the last number for speed/accuracy\n",
        "\n",
        "# Now check for skewness in all numeric columns\n",
        "for col in numeric_inputs:\n",
        "    # Calculate skewness for the column\n",
        "    skew = indexed.agg(skewness(indexed[col])).collect()  # Get skewness value\n",
        "    skew = skew[0][0]\n",
        "\n",
        "    # Debug: Print skewness value\n",
        "    print(f\"Skewness for {col}: {skew}\")\n",
        "\n",
        "    # Apply transformations based on the skewness value\n",
        "    if skew > 1:  # Right skew (positive skew)\n",
        "        indexed = indexed.withColumn(\n",
        "            col,\n",
        "            log(when(indexed[col] < d[col][0], d[col][0])  # Apply flooring\n",
        "                .when(indexed[col] > d[col][1], d[col][1])  # Apply capping\n",
        "                .otherwise(indexed[col]) + 1)  # Apply log(x + 1) for positive skew\n",
        "            .alias(col)\n",
        "        )\n",
        "        print(f\"{col} has been treated for positive (right) skewness. (skew = {skew})\")\n",
        "\n",
        "    elif skew < -1:  # Left skew (negative skew)\n",
        "        indexed = indexed.withColumn(\n",
        "            col,\n",
        "            exp(when(indexed[col] < d[col][0], d[col][0])  # Apply flooring\n",
        "                .when(indexed[col] > d[col][1], d[col][1])  # Apply capping\n",
        "                .otherwise(indexed[col]))  # Apply exp(x) for negative skew\n",
        "            .alias(col)\n",
        "        )\n",
        "        print(f\"{col} has been treated for negative (left) skewness. (skew = {skew})\")\n",
        "    else:\n",
        "        print(f\"{col} has no significant skew (skew = {skew}), no transformation applied.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mskrRILWKMJr",
        "outputId": "bbe4c08d-9273-4da1-81b8-daa1e44b9c6c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric columns to process: ['Case_No', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons', 'Qchat-10-Score', 'label']\n",
            "Skewness for Case_No: 0.0\n",
            "Case_No has no significant skew (skew = 0.0), no transformation applied.\n",
            "Skewness for A1: -0.2563496093947895\n",
            "A1 has no significant skew (skew = -0.2563496093947895), no transformation applied.\n",
            "Skewness for A2: 0.20601797730122015\n",
            "A2 has no significant skew (skew = 0.20601797730122015), no transformation applied.\n",
            "Skewness for A3: 0.40260435804137373\n",
            "A3 has no significant skew (skew = 0.40260435804137373), no transformation applied.\n",
            "Skewness for A4: -0.04935088083712881\n",
            "A4 has no significant skew (skew = -0.04935088083712881), no transformation applied.\n",
            "Skewness for A5: -0.09879203126218447\n",
            "A5 has no significant skew (skew = -0.09879203126218447), no transformation applied.\n",
            "Skewness for A6: -0.3110969802658277\n",
            "A6 has no significant skew (skew = -0.3110969802658277), no transformation applied.\n",
            "Skewness for A7: -0.6285337660559676\n",
            "A7 has no significant skew (skew = -0.6285337660559676), no transformation applied.\n",
            "Skewness for A8: 0.1637338005140266\n",
            "A8 has no significant skew (skew = 0.1637338005140266), no transformation applied.\n",
            "Skewness for A9: 0.041754827339823934\n",
            "A9 has no significant skew (skew = 0.041754827339823934), no transformation applied.\n",
            "Skewness for A10: -0.3506177559444581\n",
            "A10 has no significant skew (skew = -0.3506177559444581), no transformation applied.\n",
            "Skewness for Age_Mons: -0.6341451203248881\n",
            "Age_Mons has no significant skew (skew = -0.6341451203248881), no transformation applied.\n",
            "Skewness for Qchat-10-Score: -0.08006349388828135\n",
            "Qchat-10-Score has no significant skew (skew = -0.08006349388828135), no transformation applied.\n",
            "Skewness for label: 0.8251854531909939\n",
            "label has no significant skew (skew = 0.8251854531909939), no transformation applied.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FROM CHAT GPT**\n"
      ],
      "metadata": {
        "id": "kakUazHDSZXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from pyspark.sql.types import NumericType\n",
        "from pyspark.sql.functions import log, exp, when, skewness\n",
        "\n",
        "# Filter for numeric columns only\n",
        "numeric_inputs = [col for col, dtype in indexed.dtypes if isinstance(dtype, NumericType)]\n",
        "\n",
        "# Create empty dictionary d for quantiles\n",
        "d = {}\n",
        "\n",
        "# Create a dictionary of quantiles for numeric columns\n",
        "for col in numeric_inputs:\n",
        "    d[col] = indexed.approxQuantile(col, [0.01, 0.99], 0.25)\n",
        "\n",
        "# Check for skewness and apply transformations if needed\n",
        "for col in numeric_inputs:\n",
        "    skew = indexed.select(skewness(indexed[col])).collect()\n",
        "    skew = skew[0][0]  # Extract skewness value\n",
        "\n",
        "    # If skewness is found, apply appropriate corrections\n",
        "    if skew > 1:  # Right skew: floor, cap, and apply log(x+1)\n",
        "        indexed = indexed.withColumn(\n",
        "            col,\n",
        "            log(when(indexed[col] < d[col][0], d[col][0])\n",
        "                .when(indexed[col] > d[col][1], d[col][1])\n",
        "                .otherwise(indexed[col]) + 1).alias(col)\n",
        "        )\n",
        "        print(f\"{col} has been treated for positive (right) skewness (skew = {skew}).\")\n",
        "\n",
        "    elif skew < -1:  # Left skew: floor, cap, and apply exp(x)\n",
        "        indexed = indexed.withColumn(\n",
        "            col,\n",
        "            exp(when(indexed[col] < d[col][0], d[col][0])\n",
        "                .when(indexed[col] > d[col][1], d[col][1])\n",
        "                .otherwise(indexed[col])).alias(col)\n",
        "        )\n",
        "        print(f\"{col} has been treated for negative (left) skewness (skew = {skew}).\")\n"
      ],
      "metadata": {
        "id": "yx2U3oSrRUFV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zADjbvfrRKTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "43C5sZw3RP4F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kmjMd12wSKds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**from Chat Gpt**"
      ],
      "metadata": {
        "id": "VTFnbbCQSMYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T9Fbq-37SULJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "m70k8D7uKuBR"
      },
      "outputs": [],
      "source": [
        "# Before we correct for negative values that may have been found above,\n",
        "# We need to vectorize our df\n",
        "# becauase the function that we use to make that correction requires a vector.\n",
        "# Now create your final features list\n",
        "features_list = numeric_inputs + string_inputs\n",
        "# Create your vector assembler object\n",
        "assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
        "# And call on the vector assembler to transform your dataframe\n",
        "output = assembler.transform(indexed).select('features','label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Sbmd3P0tKuBR",
        "outputId": "7b33d5c2-6f6d-40e2-eb56-fa849c935f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features scaled to range: [0.000000, 1000.000000]\n",
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  1.0|(14,[7,8,10,11,12...|\n",
            "|  0.0|(14,[0,1,2,6,7,11...|\n",
            "|  0.0|(14,[0,1,7,8,10,1...|\n",
            "|  0.0|[2.84900284900284...|\n",
            "|  0.0|[3.79867046533713...|\n",
            "|  0.0|[4.74833808167141...|\n",
            "|  0.0|(14,[0,1,4,5,6,9,...|\n",
            "|  0.0|[6.64767331433998...|\n",
            "|  1.0|(14,[0,7,10,11,12...|\n",
            "|  0.0|[8.54700854700854...|\n",
            "|  0.0|[9.49667616334283...|\n",
            "|  0.0|[10.4463437796771...|\n",
            "|  1.0|(14,[0,11,13],[11...|\n",
            "|  0.0|[12.3456790123456...|\n",
            "|  1.0|(14,[0,11,13],[13...|\n",
            "|  0.0|[14.2450142450142...|\n",
            "|  1.0|(14,[0,11,13],[15...|\n",
            "|  0.0|[16.1443494776828...|\n",
            "|  1.0|(14,[0,1,5,10,11,...|\n",
            "|  0.0|[18.0436847103513...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the mix max scaler object\n",
        "# This is what will correct for negative values\n",
        "# I like to use a high range like 1,000\n",
        "#     because I only see one decimal place in the final_data.show() call\n",
        "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",min=0,max=1000)\n",
        "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
        "\n",
        "# Compute summary statistics and generate MinMaxScalerModel\n",
        "scalerModel = scaler.fit(output)\n",
        "\n",
        "# rescale each feature to range [min, max].\n",
        "scaled_data = scalerModel.transform(output)\n",
        "final_data = scaled_data.select('label','scaledFeatures')\n",
        "# Rename to default value\n",
        "final_data = final_data.withColumnRenamed(\"scaledFeatures\",\"features\")\n",
        "final_data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBwoau83KuBS"
      },
      "source": [
        "### Split into Test and Training datasets\n",
        "\n",
        "Now we can split into test and trainging datasets using whatever random split method we want. I will use 70/30 split but you can use your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "EzvQxZjxKuBS"
      },
      "outputs": [],
      "source": [
        "train,test = final_data.randomSplit([0.7,0.3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0EgdOFhKuBS"
      },
      "source": [
        "## Train!\n",
        "\n",
        "Now that we have our data cleaned and vectorized we are ready to feed it into our training algorithms! As we went over in the Intro to Machine Learning lecture, the building blocks of a supervised ML application consist of some data for the model to \"learn\" from. Once there is data made available, then the person building the model must decide what the apprpriate dependent and independent variables are. Then they decide which algorithms to test, and compare the performance results of each model to each other before deciding which one to select.\n",
        "\n",
        "This process usually requires several trails until a decision is reached and diligent note-taking. This first notebook will go over each of the alogorithms that PySparks offers for classification and then in a later lecture, we will go over other methods for organizing your trail and error record keeping. But for now, let's try to get a handle on the basics!\n",
        "\n",
        "This portion of the course is going to be a code review instead of a code along activity as much of the code is going to be repeatable. I thought you guys might get bored so this method will help stream line things along a bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "mRS_TdnEKuBS"
      },
      "outputs": [],
      "source": [
        "# First - Read in dependencies\n",
        "from pyspark.ml.classification import *\n",
        "from pyspark.ml.evaluation import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "itl6L8VJKuBS"
      },
      "outputs": [],
      "source": [
        "# Set up our evaluation objects\n",
        "Bin_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction') #labelCol='label'\n",
        "# Bin_evaluator = BinaryClassificationEvaluator() #labelCol='label'\n",
        "MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\","
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "P_5yfeHnKuBS",
        "outputId": "2134bd16-065e-440e-a239-046462a33ff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "label does not exist. Available: age, blood_pressure, heart_rate, sick, features, rawPrediction, probability, prediction",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-69297f0791e5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Area Under ROC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: label does not exist. Available: age, blood_pressure, heart_rate, sick, features, rawPrediction, probability, prediction"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "print('Test Area Under ROC', evaluator.evaluate(predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chat GPT\n"
      ],
      "metadata": {
        "id": "eIAVJDoDVCsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Train your model (example using LogisticRegression)\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Assuming you have your training DataFrame as 'train_data'\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
        "lr_model = lr.fit(train)\n",
        "\n",
        "# Step 2: Make predictions using the trained model\n",
        "predictions = lr_model.transform(test)  # 'test_data' is your test DataFrame\n",
        "\n",
        "# Step 3: Set up the evaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol='label',  # Column containing true labels\n",
        "    rawPredictionCol='prediction'  # Column containing predicted probabilities/scores\n",
        ")\n",
        "\n",
        "# Step 4: Calculate and print the Area Under ROC (AUC)\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print('Test Area Under ROC:', auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjU6wgLpVHkL",
        "outputId": "dd384dec-7dd9-4529-e5f4-19322205281b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Area Under ROC: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C5cIFguKuBS"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "**Review**\n",
        "The Logistic Regression Algorithm, also known as \"Logit\", is used to estimate (guess) the probability (a number between 0 and 1) of an event occurring having been given some previous data to “learn” from. It works with either binary or multinomial (more than 2 categories) data and uses logistic function (ie. log) to find a model that fits with the data points.\n",
        "\n",
        "**Example**\n",
        "You may want to predict the likelihood of a student passing or failing an exam based on a set of biographical factors. The model you create will provide a probability (i.e a number between 0 and 1) that you can use to determine the likelihood of each student passing.\n",
        "\n",
        "PySpark Documentation Link: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MocQ_SwxKuBS",
        "outputId": "8ef1957d-e9ee-4b19-e70d-95412521f896",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Area Under ROC 1.0\n"
          ]
        }
      ],
      "source": [
        "predictions = lr_model.transform(test)\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "print('Test Area Under ROC', evaluator.evaluate(predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRtEYGHVKuBT",
        "outputId": "19e3432e-5316-483a-e76b-1246f2d3ced7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 1.0\n",
            "Accuracy: 100.00 %\n",
            " \n"
          ]
        }
      ],
      "source": [
        "# This is the most simplistic approach which does not use cross validation\n",
        "# Let's go ahead and train a Logistic Regression Algorithm\n",
        "classifier = LogisticRegression()\n",
        "fitModel = classifier.fit(train)\n",
        "\n",
        "# Evaluation method for binary classification problem\n",
        "predictionAndLabels = fitModel.transform(test)\n",
        "auc = Bin_evaluator.evaluate(predictionAndLabels)\n",
        "print(\"AUC:\",auc)\n",
        "\n",
        "# Evaluation for a multiclass classification problem\n",
        "predictions = fitModel.transform(test)\n",
        "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "print(\"Accuracy: {0:.2f}\".format(accuracy),\"%\") #     print(\"Test Error = %g \" % (1.0 - accuracy))\n",
        "print(\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9A1ghqMKuBT"
      },
      "source": [
        "#### Add in cross validation\n",
        "\n",
        "Spark also has a built-in funciton called the CrossValidator to conduct cross validation which begins by splitting the training dataset into a set of \"folds\" which are used as separate training and test datasets. For example, with k=5 folds, CrossValidator will generate 5 different (training, test) dataset pairs, each of which uses 4/5 of the data for training and 1/5 for testing. To evaluate a particular Parameter (specified in the paramgrid), CrossValidator computes the average evaluation metric for the 5 Models produced by fitting the Estimator on the 5 different (training, test) dataset pairs and tells you which model performed the best once it is finished.\n",
        "\n",
        "After identifying the best ParamMap (more details can be found in the documentation link above), CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset.\n",
        "\n",
        "**MaxIter:** <br>\n",
        "The maximum number of iterations to use. There is no clear formula for setting the optimum iteration number, but you can figure out this issue by an iterative process by initializing the iteration number by a small number like 100 and then increase it linearly. This process will be repeated until the MSE of the test does not decrease and even may increase. The below link describes well:\n",
        "https://www.quora.com/What-will-happen-if-I-train-my-neural-networks-with-too-much-iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8zQpIm_hKuBT",
        "outputId": "ca57643c-2492-4297-ad0f-62b855e25423",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: [-1.6144875933719627]\n",
            "Coefficients: \n",
            "DenseMatrix([[-0.00039034, -0.00072087, -0.00075867, -0.00079391, -0.00088945,\n",
            "              -0.00071782, -0.00079028, -0.00108567, -0.00077191, -0.00092058,\n",
            "              -0.00066948, -0.00037155, -0.00234031,  0.0123923 ]])\n",
            "100.0\n"
          ]
        }
      ],
      "source": [
        "# First tell Spark which classifier you want to use\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Then Set up your parameter grid for the cross validator to conduct hyperparameter tuning\n",
        "paramGrid = (ParamGridBuilder().addGrid(classifier.maxIter, [10, 15,20]).build())\n",
        "# here 10,20,30  number of iterations\n",
        "# Then set up the Cross Validator which requires all of the following parameters:\n",
        "crossval = CrossValidator(estimator=classifier,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MC_evaluator,\n",
        "                          numFolds=2) # 3 + is best practice\n",
        "\n",
        "# Then fit your model\n",
        "fitModel = crossval.fit(train)\n",
        "\n",
        "# Collect the best model and\n",
        "# print the coefficient matrix\n",
        "# These values should be compared relative to eachother\n",
        "# And intercepts can be prepared to other models\n",
        "BestModel = fitModel.bestModel\n",
        "print(\"Intercept: \" + str(BestModel.interceptVector))\n",
        "print(\"Coefficients: \\n\" + str(BestModel.coefficientMatrix))\n",
        "\n",
        "# You can extract the best model from this run like this if you want\n",
        "LR_BestModel = BestModel\n",
        "\n",
        "# Next you need to generate predictions on the test dataset\n",
        "# fitModel automatically uses the best model\n",
        "# so we don't need to use BestModel here\n",
        "predictions = fitModel.transform(test)\n",
        "\n",
        "# Now print the accuracy rate of the model or AUC for a binary classifier\n",
        "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w2SLEZ7KuBT",
        "outputId": "171ba80f-06d8-40ad-b76d-9b559870b694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+\n",
            "|       feature|               coeff|\n",
            "+--------------+--------------------+\n",
            "|            A1|-5.56132407170836...|\n",
            "|            A2|-8.58977585087988...|\n",
            "|            A3|-9.14454210973763...|\n",
            "|            A4|-4.22477007759667...|\n",
            "|            A5|-8.26142907411533...|\n",
            "|            A6|-8.52147812024888...|\n",
            "|            A7|-7.64736460379106...|\n",
            "|            A8|-0.00100089729872...|\n",
            "|            A9|-8.12895012016068...|\n",
            "|           A10|-9.66679360866760...|\n",
            "|      Age_Mons|-6.05743178597732...|\n",
            "|Qchat-10-Score|-4.04210780288548E-4|\n",
            "|           Sex|-0.00235554016666...|\n",
            "|     Ethnicity|0.012319346606065207|\n",
            "+--------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# zip input_columns qith feature importance scores and create df\n",
        "\n",
        "# First convert featureimportance scores from numpy array to list\n",
        "coeff_array = BestModel.coefficientMatrix.toArray()\n",
        "coeff_scores = []\n",
        "for x in coeff_array[0]:\n",
        "    coeff_scores.append(float(x))\n",
        "# Then zip with input_columns list and create a df\n",
        "\n",
        "# data_schema = [StructField(\"feature\", StringType(), True),StructField(\"coeff\", DecimalType(), True)]\n",
        "# final_struc = StructType(fields=data_schema)\n",
        "# result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=final_struc)\n",
        "\n",
        "result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
        "result.show(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwwTSkGSKuBT"
      },
      "source": [
        "100% accuracy! Pretty good eh? Well this is probably just because we did not have much test data. You will almost never see this happen. If you know more about the subject matter of the data, you can compare the coefficients you see here. If you are not familiar with this concept, here is a good article to help you understand: https://www.displayr.com/how-to-interpret-logistic-regression-coefficients/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NewvQcauKuBU"
      },
      "source": [
        "### Classification Diagnostics\n",
        "\n",
        "You can also generate some more detailed diagnostics too if you want.\n",
        "\n",
        "To learn more about it please visit: https://towardsdatascience.com/demystifying-confusion-matrix-confusion-9e82201592fd. Please note that this article provides coding examples for scikit learn (not PySpark) so please do not let that confuse you. However I found the explaination and approach to be very helpful.\n",
        "\n",
        "\n",
        "*Note: This output is ONLY available for Logistic Regression*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAjhNVLmKuBU",
        "outputId": "1c665311-b122-4494-cfcd-c68b3d02f09c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+\n",
            "|summary|              label|         prediction|\n",
            "+-------+-------------------+-------------------+\n",
            "|  count|                746|                746|\n",
            "|   mean| 0.3042895442359249| 0.3042895442359249|\n",
            "| stddev|0.46041456874430764|0.46041456874430764|\n",
            "|    min|                0.0|                0.0|\n",
            "|    max|                1.0|                1.0|\n",
            "+-------+-------------------+-------------------+\n",
            "\n",
            " \n",
            "objectiveHistory: (scaled loss + regularization) at each iteration\n",
            "0.6144551319311358\n",
            "0.16460825629746517\n",
            "0.1305611931524646\n",
            "0.056880454132079206\n",
            "0.02804175289228191\n",
            "0.013366639454708867\n",
            "0.006612525220693942\n",
            "0.0032706968873543038\n",
            "0.0016294148677586538\n",
            "0.0008124082280944698\n",
            "0.0004055612705331176\n",
            " \n",
            "False positive rate by label:\n",
            "label 0: 0.0\n",
            "label 1: 0.0\n",
            " \n",
            "True positive rate by label:\n",
            "label 0: 1.0\n",
            "label 1: 1.0\n",
            " \n",
            "Precision by label:\n",
            "label 0: 1.0\n",
            "label 1: 1.0\n",
            " \n",
            "Recall by label:\n",
            "label 0: 1.0\n",
            "label 1: 1.0\n",
            " \n",
            "F-measure by label:\n",
            "label 0: 1.0\n",
            "label 1: 1.0\n",
            " \n",
            "Accuracy: 1.0\n",
            "FPR: 0.0\n",
            "TPR: 1.0\n",
            "F-measure: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Load the Summary\n",
        "trainingSummary = LR_BestModel.summary\n",
        "\n",
        "# General Describe\n",
        "trainingSummary.predictions.describe().show()\n",
        "\n",
        "# Obtain the objective per iteration\n",
        "objectiveHistory = trainingSummary.objectiveHistory\n",
        "print(\" \")\n",
        "print(\"objectiveHistory: (scaled loss + regularization) at each iteration\")\n",
        "for objective in objectiveHistory:\n",
        "    print(objective)\n",
        "\n",
        "# for multiclass, we can inspect metrics on a per-label basis\n",
        "print(\" \")\n",
        "print(\"False positive rate by label:\")\n",
        "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
        "    print(\"label %d: %s\" % (i, rate))\n",
        "\n",
        "print(\" \")\n",
        "print(\"True positive rate by label:\")\n",
        "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
        "    print(\"label %d: %s\" % (i, rate))\n",
        "\n",
        "print(\" \")\n",
        "print(\"Precision by label:\")\n",
        "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
        "    print(\"label %d: %s\" % (i, prec))\n",
        "\n",
        "print(\" \")\n",
        "print(\"Recall by label:\")\n",
        "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
        "    print(\"label %d: %s\" % (i, rec))\n",
        "\n",
        "print(\" \")\n",
        "print(\"F-measure by label:\")\n",
        "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
        "    print(\"label %d: %s\" % (i, f))\n",
        "\n",
        "# Generate confusion matrix and print (includes accuracy)\n",
        "accuracy = trainingSummary.accuracy\n",
        "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
        "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
        "fMeasure = trainingSummary.weightedFMeasure()\n",
        "precision = trainingSummary.weightedPrecision\n",
        "recall = trainingSummary.weightedRecall\n",
        "print(\" \")\n",
        "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
        "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2qtF_zEKuBU"
      },
      "source": [
        "### One vs. Rest\n",
        "\n",
        "**Recap from lecture**\n",
        "The One-vs-Rest classifier is a type of multiclass classifier that involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. So each class is viewed as it compares to rest of the classes as a whole, as opposed to each one individually.\n",
        "\n",
        "**regParam**: <br>\n",
        "The purpose of the regularizer is to encourage simple models and avoid overfitting. To learn more about this concept, here is an interesting article: https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a\n",
        "\n",
        "\n",
        "PySpark Documentation link: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.OneVsRest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "VQSc36xtKuBU",
        "outputId": "d8b07cde-028a-497b-9764-a806038122a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mIntercept: \u001b[0m -1.1980249685411162 \u001b[1m\n",
            "Coefficients:\u001b[0m [0.00014902022575136428,0.0004759852475069171,0.00046981460492512934,0.00042161489282809743,0.00048759813916896184,0.0005255558721592255,0.0005576351965753215,0.0006393775042343804,0.00045330316943260267,0.0005753270514516996,0.000309689471533936,0.0001517219399132898,0.001418343867977338,-0.00197715255655174]\n",
            "\u001b[1mIntercept: \u001b[0m 1.1980249685411162 \u001b[1m\n",
            "Coefficients:\u001b[0m [-0.00014902022575136482,-0.0004759852475069169,-0.00046981460492512907,-0.0004216148928280976,-0.00048759813916896184,-0.0005255558721592254,-0.0005576351965753212,-0.0006393775042343803,-0.0004533031694326028,-0.0005753270514516995,-0.0003096894715339361,-0.00015172193991328913,-0.001418343867977338,0.0019771525565517394]\n",
            "100.0\n"
          ]
        }
      ],
      "source": [
        "# instantiate the base classifier.\n",
        "lr = LogisticRegression()\n",
        "# instantiate the One Vs Rest Classifier.\n",
        "classifier = OneVsRest(classifier=lr)\n",
        "\n",
        "# Add parameters of your choice here:\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .build()\n",
        "#Cross Validator requires the following parameters:\n",
        "crossval = CrossValidator(estimator=classifier,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(),\n",
        "                          numFolds=2) # 3 is best practice\n",
        "\n",
        "# Run cross-validation, and choose the best set of parameters.\n",
        "fitModel = crossval.fit(train)\n",
        "\n",
        "# Print the Coefficients\n",
        "# First we need to extract the best model from fit model\n",
        "\n",
        "# Get Best Model\n",
        "BestModel = fitModel.bestModel\n",
        "# Extract list of binary models\n",
        "models = BestModel.models\n",
        "for model in models:\n",
        "    print('\\033[1m' + 'Intercept: '+ '\\033[0m',model.intercept,'\\033[1m' + '\\nCoefficients:'+ '\\033[0m',model.coefficients)\n",
        "\n",
        "# Now generate predictions on test dataset\n",
        "predictions = fitModel.transform(test)\n",
        "# And calculate the accuracy score\n",
        "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "# And print\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPIUUG5BKuBU",
        "outputId": "0ad961d9-c499-4705-b35a-aa06eb0aef75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mModel Weights: \u001b[0m 479\n",
            "Accuracy:  100.0\n"
          ]
        }
      ],
      "source": [
        "# Count how many features you have\n",
        "features = final_data.select(['features']).collect()\n",
        "features_count = len(features[0][0])\n",
        "# Count how many classes you have\n",
        "class_count = final_data.select(countDistinct(\"label\")).collect()\n",
        "classes = class_count[0][0]\n",
        "\n",
        "# Then use this number to specify the layers\n",
        "# The first number in this list is the input layer which has to be equal to the number of features in your vector\n",
        "# The second number is the first hidden layer\n",
        "# The third number is the second hidden layer\n",
        "# The fourth number is the output layer which has to be equal to your class size\n",
        "layers = [features_count, features_count+1, features_count, classes]\n",
        "# Instaniate the classifier\n",
        "classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
        "\n",
        "# Fit the model\n",
        "fitModel = classifier.fit(train)\n",
        "\n",
        "# Print the model Weights\n",
        "print('\\033[1m' + \"Model Weights: \"+ '\\033[0m',fitModel.weights.size)\n",
        "\n",
        "# Generate predictions on test dataframe\n",
        "predictions = fitModel.transform(test)\n",
        "# Calculate accuracy score\n",
        "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "# Print accuracy score\n",
        "print(\"Accuracy: \",accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiMNDLJAKuBU"
      },
      "source": [
        "## Multilayer Perceptron Classifier\n",
        "\n",
        "*Neural Network* <br>\n",
        "\n",
        "**Recap from the lecture** <br>\n",
        "A multilayer perceptron (MLP) is a class of feedforward artificial neural network. It consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.\n",
        "\n",
        "#### Common Hyper Parameters:\n",
        "\n",
        "**MaxIter:** <br>\n",
        "The maximum number of iterations to use. There is no clear formula for setting the optimum iteration number, but you can figure out this issue by an iterative process by initializing the iteration number by a small number like 100 and then increase it linearly. This process will be repeated until the MSE of the test does not decrease and even may increase. The below link describes well:\n",
        "https://www.quora.com/What-will-happen-if-I-train-my-neural-networks-with-too-much-iteration\n",
        "\n",
        "**Layers:** <br>\n",
        "Spark requires that the input layer equals the number of features in the dataset, the hidden layer might be one or two more than that (flexible), and the output layer has to be equal to the number of classes. Here's a great article to learn more about how to play around with the hidden layers: https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e\n",
        "\n",
        "**Block size:** <br>\n",
        "Block size for stacking input data in matrices to speed up the computation. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. Recommended size is between 10 and 1000. Default: 128\n",
        "\n",
        "**Seed:** <br>\n",
        "A random seed. Set this value if you need your results to be reproducible across repeated calls (highly recommdended).\n",
        "\n",
        "**Weights**: *printed for us below along with accuracy rate* <br>\n",
        "Each hidden neuron added will increase the number of weights, thus it is recommended to use the least number of hidden neurons that accomplish the task. Using more hidden neurons than required will add more complexity.\n",
        "\n",
        "**PySpark Documentation link:** <br>\n",
        "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.MultilayerPerceptronClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many features you have\n",
        "features = final_data.select(['features']).collect()\n",
        "print(features)\n",
        "features_count = len(features[0][0])\n",
        "# Count how many classes you have\n",
        "class_count = final_data.select(countDistinct(\"label\")).collect()\n",
        "classes = class_count[0][0]\n",
        "\n",
        "# Then use this number to specify the layers\n",
        "# The first number in this list is the input layer which has to be equal to the number of features in your vector\n",
        "# The second number is the first hidden layer\n",
        "# The third number is the second hidden layer\n",
        "# The fourth number is the output layer which has to be equal to your class size\n",
        "layers = [features_count, features_count+1, features_count, classes]\n",
        "# Instaniate the classifier\n",
        "classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
        "\n",
        "# Fit the model\n",
        "fitModel = classifier.fit(train)\n",
        "\n",
        "# Print the model Weights\n",
        "print('\\033[1m' + \"Model Weights: \"+ '\\033[0m',fitModel.weights.size)\n",
        "\n",
        "# Generate predictions on test dataframe\n",
        "predictions = fitModel.transform(test)\n",
        "# Calculate accuracy score\n",
        "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "# Print accuracy score\n",
        "print(\"Accuracy: \",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i49PkCTAX5vZ",
        "outputId": "eb4492b5-b45f-4b5f-84dc-6eb51f620505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(features=SparseVector(14, {7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 666.6667, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 0.9497, 1: 1000.0, 2: 1000.0, 6: 1000.0, 7: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 1.8993, 1: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([2.849, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 1000.0, 0.0])), Row(features=DenseVector([3.7987, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 333.3333, 900.0, 0.0])), Row(features=DenseVector([4.7483, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 375.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 5.698, 1: 1000.0, 4: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 11: 875.0, 12: 500.0})), Row(features=DenseVector([6.6477, 0.0, 1000.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 875.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 7.5973, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([8.547, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 416.6667, 800.0, 0.0])), Row(features=DenseVector([9.4967, 1000.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=DenseVector([10.4463, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 208.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 11.396, 11: 541.6667, 13: 1000.0})), Row(features=DenseVector([12.3457, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 125.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 13.2953, 11: 250.0, 13: 1000.0})), Row(features=DenseVector([14.245, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 15.1947, 11: 1000.0, 13: 1000.0})), Row(features=DenseVector([16.1443, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 17.094, 1: 1000.0, 5: 1000.0, 10: 1000.0, 11: 708.3333, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([18.0437, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 700.0, 0.0])), Row(features=DenseVector([18.9934, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([19.943, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([20.8927, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([21.8424, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([22.792, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 416.6667, 900.0, 0.0])), Row(features=SparseVector(14, {0: 23.7417, 11: 500.0, 13: 1000.0})), Row(features=DenseVector([24.6914, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([25.641, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 958.3333, 1000.0, 0.0])), Row(features=DenseVector([26.5907, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 27.5404, 11: 833.3333, 13: 1000.0})), Row(features=SparseVector(14, {0: 28.49, 11: 250.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 29.4397, 13: 1000.0})), Row(features=SparseVector(14, {0: 30.3894, 10: 1000.0, 11: 125.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 31.339, 13: 1000.0})), Row(features=SparseVector(14, {0: 32.2887, 10: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([33.2384, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 125.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 34.188, 10: 1000.0, 11: 125.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 35.1377, 11: 125.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 36.0874, 11: 125.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 37.037, 10: 1000.0, 11: 125.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 37.9867, 10: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 38.9364, 11: 125.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 39.886, 11: 125.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 40.8357, 11: 125.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 41.7854, 10: 1000.0, 11: 125.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 42.735, 7: 1000.0, 10: 1000.0, 11: 291.6667, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 43.6847, 10: 1000.0, 11: 250.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 44.6344, 10: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 45.584, 10: 1000.0, 11: 83.3333, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 46.5337, 10: 1000.0, 11: 83.3333, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 47.4834, 10: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 48.433, 10: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 49.3827, 10: 1000.0, 11: 125.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([50.3324, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 51.2821, 10: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([52.2317, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 53.1814, 1: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 833.3333, 12: 500.0})), Row(features=DenseVector([54.1311, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 541.6667, 600.0, 0.0])), Row(features=SparseVector(14, {0: 55.0807, 8: 1000.0, 10: 1000.0, 11: 125.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 56.0304, 10: 1000.0, 11: 41.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 56.9801, 10: 1000.0, 11: 41.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 57.9297, 10: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([58.8794, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 59.8291, 3: 1000.0, 4: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 750.0, 12: 500.0})), Row(features=SparseVector(14, {0: 60.7787, 3: 1000.0, 4: 1000.0, 9: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([61.7284, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 62.6781, 5: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 63.6277, 8: 1000.0, 11: 708.3333, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 64.5774, 1: 1000.0, 3: 1000.0, 11: 458.3333, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([65.5271, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 250.0, 800.0, 0.0])), Row(features=DenseVector([66.4767, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 916.6667, 600.0, 0.0])), Row(features=DenseVector([67.4264, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 916.6667, 700.0, 0.0])), Row(features=DenseVector([68.3761, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 375.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 69.3257, 8: 1000.0, 11: 750.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 70.2754, 1: 1000.0, 2: 1000.0, 5: 1000.0, 7: 1000.0, 11: 583.3333, 12: 400.0})), Row(features=DenseVector([71.2251, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 291.6667, 700.0, 0.0])), Row(features=SparseVector(14, {0: 72.1747, 11: 1000.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 73.1244, 11: 1000.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 74.0741, 11: 1000.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 75.0237, 1: 1000.0, 8: 1000.0, 10: 1000.0, 11: 750.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 75.9734, 2: 1000.0, 10: 1000.0, 11: 750.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([76.9231, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 708.3333, 700.0, 0.0])), Row(features=DenseVector([77.8727, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([78.8224, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 750.0, 700.0, 0.0])), Row(features=DenseVector([79.7721, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 750.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 80.7217, 4: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 10: 1000.0, 11: 458.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 81.6714, 11: 416.6667, 13: 1000.0})), Row(features=SparseVector(14, {0: 82.6211, 4: 1000.0, 7: 1000.0, 10: 1000.0, 11: 333.3333, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 83.5708, 1: 1000.0, 5: 1000.0, 7: 1000.0, 9: 1000.0, 11: 916.6667, 12: 400.0})), Row(features=DenseVector([84.5204, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 916.6667, 900.0, 0.0])), Row(features=SparseVector(14, {0: 85.4701, 10: 1000.0, 11: 916.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 86.4198, 1: 1000.0, 7: 1000.0, 10: 1000.0, 11: 916.6667, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([87.3694, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 833.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 88.3191, 8: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 89.2688, 4: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 10: 1000.0, 11: 875.0, 12: 500.0})), Row(features=SparseVector(14, {0: 90.2184, 2: 1000.0, 10: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([91.1681, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 500.0, 800.0, 0.0])), Row(features=DenseVector([92.1178, 1000.0, 0.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 916.6667, 600.0, 0.0])), Row(features=SparseVector(14, {0: 93.0674, 10: 1000.0, 11: 916.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 94.0171, 10: 1000.0, 11: 791.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 94.9668, 3: 1000.0, 4: 1000.0, 11: 500.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 95.9164, 6: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 791.6667, 12: 400.0})), Row(features=SparseVector(14, {0: 96.8661, 1: 1000.0, 4: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 791.6667, 12: 500.0})), Row(features=SparseVector(14, {0: 97.8158, 1: 1000.0, 2: 1000.0, 8: 1000.0, 9: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 98.7654, 1: 1000.0, 2: 1000.0, 6: 1000.0, 10: 1000.0, 11: 791.6667, 12: 400.0})), Row(features=DenseVector([99.7151, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 541.6667, 900.0, 0.0])), Row(features=SparseVector(14, {0: 100.6648, 10: 1000.0, 11: 458.3333, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([101.6144, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 500.0, 600.0, 0.0])), Row(features=DenseVector([102.5641, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 103.5138, 10: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 104.4634, 5: 1000.0, 7: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 105.4131, 2: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 106.3628, 7: 1000.0, 8: 1000.0, 11: 833.3333, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 107.3124, 7: 1000.0, 8: 1000.0, 11: 833.3333, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([108.2621, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 750.0, 700.0, 0.0])), Row(features=DenseVector([109.2118, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 833.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 110.1614, 3: 1000.0, 4: 1000.0, 7: 1000.0, 10: 1000.0, 11: 833.3333, 12: 400.0})), Row(features=DenseVector([111.1111, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 708.3333, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 112.0608, 1: 1000.0, 2: 1000.0, 6: 1000.0, 11: 666.6667, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([113.0104, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 0.0, 1000.0, 250.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 113.9601, 2: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 10: 1000.0, 11: 625.0, 12: 500.0})), Row(features=SparseVector(14, {0: 114.9098, 2: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 625.0, 12: 500.0})), Row(features=SparseVector(14, {0: 115.8594, 8: 1000.0, 11: 583.3333, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 116.8091, 6: 1000.0, 11: 541.6667, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([117.7588, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 291.6667, 800.0, 0.0])), Row(features=DenseVector([118.7085, 1000.0, 0.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 119.6581, 10: 1000.0, 11: 541.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 120.6078, 10: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([121.5575, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 208.3333, 600.0, 0.0])), Row(features=DenseVector([122.5071, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([123.4568, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([124.4065, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 333.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 125.3561, 11: 875.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 126.3058, 1: 1000.0, 5: 1000.0, 11: 875.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 127.2555, 1: 1000.0, 2: 1000.0, 5: 1000.0, 8: 1000.0, 11: 875.0, 12: 400.0})), Row(features=SparseVector(14, {0: 128.2051, 5: 1000.0, 11: 916.6667, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([129.1548, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 130.1045, 3: 1000.0, 4: 1000.0, 5: 1000.0, 9: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([131.0541, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 958.3333, 900.0, 0.0])), Row(features=SparseVector(14, {0: 132.0038, 1: 1000.0, 2: 1000.0, 8: 1000.0, 9: 1000.0, 11: 583.3333, 12: 400.0})), Row(features=DenseVector([132.9535, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 458.3333, 900.0, 0.0])), Row(features=DenseVector([133.9031, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 500.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 134.8528, 1: 1000.0, 2: 1000.0, 6: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 135.8025, 2: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 136.7521, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 583.3333, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([137.7018, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 458.3333, 900.0, 0.0])), Row(features=DenseVector([138.6515, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 458.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 139.6011, 10: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 140.5508, 7: 1000.0, 11: 750.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 141.5005, 1: 1000.0, 8: 1000.0, 11: 291.6667, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 142.4501, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 143.3998, 2: 1000.0, 4: 1000.0, 7: 1000.0, 10: 1000.0, 11: 875.0, 12: 400.0})), Row(features=SparseVector(14, {0: 144.3495, 4: 1000.0, 7: 1000.0, 10: 1000.0, 11: 875.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([145.2991, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 583.3333, 900.0, 0.0])), Row(features=SparseVector(14, {0: 146.2488, 1: 1000.0, 2: 1000.0, 4: 1000.0, 7: 1000.0, 10: 1000.0, 11: 875.0, 12: 500.0})), Row(features=DenseVector([147.1985, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=DenseVector([148.1481, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 500.0, 900.0, 0.0])), Row(features=DenseVector([149.0978, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 666.6667, 600.0, 0.0])), Row(features=DenseVector([150.0475, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=DenseVector([150.9972, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 800.0, 0.0])), Row(features=DenseVector([151.9468, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 458.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 152.8965, 1: 1000.0, 3: 1000.0, 4: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 12: 600.0})), Row(features=DenseVector([153.8462, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 708.3333, 900.0, 0.0])), Row(features=SparseVector(14, {0: 154.7958, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 416.6667, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 155.7455, 7: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 156.6952, 10: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 157.6448, 2: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 158.5945, 7: 1000.0, 11: 750.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 159.5442, 11: 750.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 160.4938, 11: 1000.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 161.4435, 4: 1000.0, 5: 1000.0, 6: 1000.0, 11: 291.6667, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([162.3932, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 458.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 163.3428, 10: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 164.2925, 2: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 165.2422, 2: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 166.1918, 1: 1000.0, 5: 1000.0, 6: 1000.0, 8: 1000.0, 10: 1000.0, 11: 875.0, 12: 500.0})), Row(features=SparseVector(14, {0: 167.1415, 3: 1000.0, 5: 1000.0, 10: 1000.0, 11: 875.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 168.0912, 3: 1000.0, 6: 1000.0, 11: 875.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 169.0408, 1: 1000.0, 3: 1000.0, 6: 1000.0, 11: 875.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 169.9905, 1: 1000.0, 3: 1000.0, 6: 1000.0, 11: 875.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 170.9402, 2: 1000.0, 4: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 171.8898, 4: 1000.0, 11: 750.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 172.8395, 1: 1000.0, 6: 1000.0, 9: 1000.0, 11: 875.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 173.7892, 1: 1000.0, 3: 1000.0, 4: 1000.0, 11: 750.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([174.7388, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 958.3333, 900.0, 0.0])), Row(features=SparseVector(14, {0: 175.6885, 5: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 12: 400.0})), Row(features=DenseVector([176.6382, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 177.5878, 1: 1000.0, 2: 1000.0, 10: 1000.0, 11: 833.3333, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 178.5375, 2: 1000.0, 10: 1000.0, 11: 791.6667, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 179.4872, 1: 1000.0, 2: 1000.0, 10: 1000.0, 11: 791.6667, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 180.4368, 1: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 500.0, 12: 500.0})), Row(features=SparseVector(14, {0: 181.3865, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 625.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 182.3362, 1: 1000.0, 2: 1000.0, 7: 1000.0, 10: 1000.0, 11: 625.0, 12: 400.0})), Row(features=SparseVector(14, {0: 183.2858, 4: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 184.2355, 1: 1000.0, 4: 1000.0, 6: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([185.1852, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([186.1349, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 600.0, 0.0])), Row(features=DenseVector([187.0845, 1000.0, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 333.3333, 700.0, 0.0])), Row(features=DenseVector([188.0342, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 333.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 188.9839, 2: 1000.0, 3: 1000.0, 5: 1000.0, 7: 1000.0, 10: 1000.0, 11: 958.3333, 12: 500.0})), Row(features=DenseVector([189.9335, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=DenseVector([190.8832, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 900.0, 0.0])), Row(features=DenseVector([191.8329, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 750.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 192.7825, 1: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 916.6667, 12: 400.0})), Row(features=DenseVector([193.7322, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 194.6819, 1: 1000.0, 2: 1000.0, 7: 1000.0, 10: 1000.0, 11: 125.0, 12: 400.0})), Row(features=DenseVector([195.6315, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 196.5812, 4: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 197.5309, 2: 1000.0, 4: 1000.0, 6: 1000.0, 11: 125.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 198.4805, 5: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 199.4302, 2: 1000.0, 3: 1000.0, 4: 1000.0, 11: 750.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([200.3799, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([201.3295, 0.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([202.2792, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 166.6667, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 203.2289, 7: 1000.0, 8: 1000.0, 9: 1000.0, 10: 1000.0, 11: 541.6667, 12: 400.0})), Row(features=SparseVector(14, {0: 204.1785, 4: 1000.0, 8: 1000.0, 10: 1000.0, 11: 958.3333, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 205.1282, 6: 1000.0, 9: 1000.0, 10: 1000.0, 11: 41.6667, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([206.0779, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 541.6667, 900.0, 0.0])), Row(features=DenseVector([207.0275, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 416.6667, 700.0, 0.0])), Row(features=DenseVector([207.9772, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 458.3333, 800.0, 0.0])), Row(features=DenseVector([208.9269, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 209.8765, 10: 1000.0, 11: 666.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 210.8262, 1: 1000.0, 2: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([211.7759, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 375.0, 900.0, 0.0])), Row(features=DenseVector([212.7255, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 416.6667, 900.0, 0.0])), Row(features=SparseVector(14, {0: 213.6752, 5: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 750.0, 12: 400.0})), Row(features=DenseVector([214.6249, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 41.6667, 700.0, 0.0])), Row(features=DenseVector([215.5745, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 333.3333, 700.0, 0.0])), Row(features=DenseVector([216.5242, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 416.6667, 900.0, 0.0])), Row(features=SparseVector(14, {0: 217.4739, 1: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 875.0, 12: 500.0})), Row(features=SparseVector(14, {0: 218.4236, 1: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 875.0, 12: 500.0})), Row(features=DenseVector([219.3732, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=DenseVector([220.3229, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 333.3333, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 221.2726, 2: 1000.0, 4: 1000.0, 5: 1000.0, 9: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([222.2222, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 223.1719, 7: 1000.0, 10: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([224.1216, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 225.0712, 11: 791.6667, 13: 1000.0})), Row(features=SparseVector(14, {0: 226.0209, 1: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 11: 791.6667, 12: 500.0})), Row(features=SparseVector(14, {0: 226.9706, 6: 1000.0, 7: 1000.0, 8: 1000.0, 11: 791.6667, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([227.9202, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 125.0, 1000.0, 0.0])), Row(features=DenseVector([228.8699, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 291.6667, 700.0, 0.0])), Row(features=DenseVector([229.8196, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 291.6667, 700.0, 0.0])), Row(features=SparseVector(14, {0: 230.7692, 10: 1000.0, 11: 291.6667, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([231.7189, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 0.0, 500.0, 600.0, 0.0])), Row(features=DenseVector([232.6686, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 800.0, 0.0])), Row(features=DenseVector([233.6182, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=DenseVector([234.5679, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 800.0, 0.0])), Row(features=DenseVector([235.5176, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 250.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 236.4672, 5: 1000.0, 6: 1000.0, 11: 583.3333, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([237.4169, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 238.3666, 5: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 11: 458.3333, 12: 400.0})), Row(features=DenseVector([239.3162, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([240.2659, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([241.2156, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 791.6667, 900.0, 0.0])), Row(features=DenseVector([242.1652, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 791.6667, 900.0, 0.0])), Row(features=DenseVector([243.1149, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 958.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 244.0646, 2: 1000.0, 8: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([245.0142, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 458.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 245.9639, 1: 1000.0, 6: 1000.0, 7: 1000.0, 11: 625.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([246.9136, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 875.0, 700.0, 0.0])), Row(features=DenseVector([247.8632, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 248.8129, 1: 1000.0, 3: 1000.0, 4: 1000.0, 6: 1000.0, 9: 1000.0, 11: 750.0, 12: 500.0})), Row(features=SparseVector(14, {0: 249.7626, 1: 1000.0, 3: 1000.0, 4: 1000.0, 6: 1000.0, 11: 750.0, 12: 400.0})), Row(features=SparseVector(14, {0: 250.7123, 11: 500.0, 13: 1000.0})), Row(features=DenseVector([251.6619, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 166.6667, 700.0, 0.0])), Row(features=SparseVector(14, {0: 252.6116, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([253.5613, 0.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 254.5109, 4: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([255.4606, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 256.4103, 11: 1000.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 257.3599, 1: 1000.0, 2: 1000.0, 4: 1000.0, 7: 1000.0, 10: 1000.0, 11: 708.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 258.3096, 1: 1000.0, 3: 1000.0, 6: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 259.2593, 1: 1000.0, 3: 1000.0, 6: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([260.2089, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 833.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 261.1586, 1: 1000.0, 2: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 833.3333, 12: 500.0})), Row(features=DenseVector([262.1083, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([263.0579, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 264.0076, 10: 1000.0, 11: 541.6667, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([264.9573, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 900.0, 0.0])), Row(features=DenseVector([265.9069, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 1000.0, 0.0])), Row(features=DenseVector([266.8566, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 625.0, 900.0, 0.0])), Row(features=DenseVector([267.8063, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 125.0, 700.0, 0.0])), Row(features=DenseVector([268.7559, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=DenseVector([269.7056, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 800.0, 0.0])), Row(features=SparseVector(14, {0: 270.6553, 1: 1000.0, 2: 1000.0, 10: 1000.0, 11: 500.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 271.6049, 5: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 791.6667, 12: 400.0})), Row(features=DenseVector([272.5546, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 458.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 273.5043, 1: 1000.0, 2: 1000.0, 7: 1000.0, 9: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 274.4539, 1: 1000.0, 2: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 583.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 275.4036, 7: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([276.3533, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 333.3333, 600.0, 0.0])), Row(features=SparseVector(14, {0: 277.3029, 4: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 333.3333, 12: 400.0})), Row(features=DenseVector([278.2526, 1000.0, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 791.6667, 600.0, 0.0])), Row(features=SparseVector(14, {0: 279.2023, 2: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 280.1519, 11: 833.3333, 13: 1000.0})), Row(features=SparseVector(14, {0: 281.1016, 1: 1000.0, 5: 1000.0, 10: 1000.0, 11: 875.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([282.0513, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([283.0009, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 208.3333, 900.0, 0.0])), Row(features=DenseVector([283.9506, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 500.0, 800.0, 0.0])), Row(features=DenseVector([284.9003, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 250.0, 800.0, 0.0])), Row(features=DenseVector([285.85, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 583.3333, 900.0, 0.0])), Row(features=DenseVector([286.7996, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 458.3333, 700.0, 0.0])), Row(features=DenseVector([287.7493, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 375.0, 800.0, 0.0])), Row(features=DenseVector([288.699, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 289.6486, 8: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 290.5983, 5: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 875.0, 12: 500.0})), Row(features=DenseVector([291.548, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 875.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 292.4976, 1: 1000.0, 2: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 293.4473, 1: 1000.0, 2: 1000.0, 3: 1000.0, 5: 1000.0, 9: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([294.397, 0.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 295.3466, 1: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 833.3333, 12: 500.0})), Row(features=DenseVector([296.2963, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 583.3333, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 297.246, 11: 1000.0, 13: 1000.0})), Row(features=DenseVector([298.1956, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 583.3333, 600.0, 0.0])), Row(features=DenseVector([299.1453, 0.0, 0.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 583.3333, 600.0, 0.0])), Row(features=DenseVector([300.095, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 83.3333, 700.0, 0.0])), Row(features=DenseVector([301.0446, 0.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 166.6667, 700.0, 0.0])), Row(features=SparseVector(14, {0: 301.9943, 1: 1000.0, 2: 1000.0, 4: 1000.0, 8: 1000.0, 10: 1000.0, 11: 666.6667, 12: 500.0})), Row(features=SparseVector(14, {0: 302.944, 11: 500.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 303.8936, 11: 375.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 304.8433, 2: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 750.0, 12: 500.0})), Row(features=DenseVector([305.793, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0])), Row(features=DenseVector([306.7426, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 833.3333, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 307.6923, 1: 1000.0, 2: 1000.0, 5: 1000.0, 7: 1000.0, 11: 791.6667, 12: 400.0})), Row(features=SparseVector(14, {0: 308.642, 1: 1000.0, 2: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 458.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 309.5916, 1: 1000.0, 4: 1000.0, 6: 1000.0, 11: 708.3333, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([310.5413, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 0.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 311.491, 10: 1000.0, 11: 41.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 312.4406, 1: 1000.0, 3: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 11: 458.3333, 12: 500.0})), Row(features=DenseVector([313.3903, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([314.34, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=DenseVector([315.2896, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 0.0, 583.3333, 600.0, 0.0])), Row(features=SparseVector(14, {0: 316.2393, 1: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 317.189, 2: 1000.0, 3: 1000.0, 4: 1000.0, 8: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 318.1387, 2: 1000.0, 4: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 319.0883, 4: 1000.0, 8: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 320.038, 4: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([320.9877, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 875.0, 1000.0, 0.0])), Row(features=DenseVector([321.9373, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=DenseVector([322.887, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 1000.0, 0.0])), Row(features=DenseVector([323.8367, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([324.7863, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 1000.0, 0.0])), Row(features=DenseVector([325.736, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 900.0, 0.0])), Row(features=DenseVector([326.6857, 0.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 600.0, 0.0])), Row(features=DenseVector([327.6353, 0.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 328.585, 1: 1000.0, 2: 1000.0, 5: 1000.0, 7: 1000.0, 8: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([329.5347, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 330.4843, 2: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 331.434, 10: 1000.0, 11: 41.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 332.3837, 10: 1000.0, 11: 41.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 333.3333, 10: 1000.0, 11: 41.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 334.283, 11: 1000.0, 13: 1000.0})), Row(features=DenseVector([335.2327, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 0.0, 916.6667, 600.0, 0.0])), Row(features=SparseVector(14, {0: 336.1823, 3: 1000.0, 4: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([337.132, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 500.0, 800.0, 0.0])), Row(features=DenseVector([338.0817, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 83.3333, 700.0, 0.0])), Row(features=DenseVector([339.0313, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 83.3333, 700.0, 0.0])), Row(features=DenseVector([339.981, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 340.9307, 11: 291.6667, 13: 1000.0})), Row(features=SparseVector(14, {0: 341.8803, 4: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 916.6667, 12: 400.0})), Row(features=DenseVector([342.83, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 343.7797, 1: 1000.0, 3: 1000.0, 4: 1000.0, 5: 1000.0, 9: 1000.0, 11: 833.3333, 12: 500.0})), Row(features=DenseVector([344.7293, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([345.679, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 346.6287, 3: 1000.0, 4: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([347.5783, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=DenseVector([348.528, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=DenseVector([349.4777, 1000.0, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([350.4274, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 625.0, 600.0, 0.0])), Row(features=DenseVector([351.377, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0])), Row(features=DenseVector([352.3267, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 208.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 353.2764, 3: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 9: 1000.0, 11: 958.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 354.226, 3: 1000.0, 4: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 11: 750.0, 12: 500.0})), Row(features=SparseVector(14, {0: 355.1757, 6: 1000.0, 10: 1000.0, 11: 875.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 356.1254, 4: 1000.0, 5: 1000.0, 7: 1000.0, 10: 1000.0, 11: 750.0, 12: 400.0})), Row(features=DenseVector([357.075, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([358.0247, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 750.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 358.9744, 4: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 750.0, 12: 400.0})), Row(features=SparseVector(14, {0: 359.924, 1: 1000.0, 7: 1000.0, 10: 1000.0, 11: 750.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 360.8737, 1: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 750.0, 12: 400.0})), Row(features=DenseVector([361.8234, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 500.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 362.773, 11: 1000.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 363.7227, 6: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 364.6724, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 666.6667, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 365.622, 4: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 666.6667, 12: 400.0})), Row(features=DenseVector([366.5717, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 583.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 367.5214, 3: 1000.0, 4: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 368.471, 5: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 369.4207, 1: 1000.0, 6: 1000.0, 8: 1000.0, 11: 541.6667, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([370.3704, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 833.3333, 900.0, 0.0])), Row(features=SparseVector(14, {0: 371.32, 1: 1000.0, 2: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([372.2697, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 373.2194, 6: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([374.169, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 375.1187, 11: 1000.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 376.0684, 2: 1000.0, 3: 1000.0, 6: 1000.0, 9: 1000.0, 10: 1000.0, 11: 333.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 377.018, 1: 1000.0, 5: 1000.0, 6: 1000.0, 8: 1000.0, 10: 1000.0, 11: 583.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 377.9677, 5: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 833.3333, 12: 400.0})), Row(features=DenseVector([378.9174, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 500.0, 700.0, 0.0])), Row(features=DenseVector([379.867, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 958.3333, 900.0, 0.0])), Row(features=SparseVector(14, {0: 380.8167, 2: 1000.0, 10: 1000.0, 11: 41.6667, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([381.7664, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 500.0, 700.0, 0.0])), Row(features=DenseVector([382.716, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 500.0, 900.0, 0.0])), Row(features=DenseVector([383.6657, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([384.6154, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 385.5651, 11: 750.0, 13: 1000.0})), Row(features=DenseVector([386.5147, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 387.4644, 4: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 875.0, 12: 400.0})), Row(features=DenseVector([388.4141, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([389.3637, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 625.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 390.3134, 5: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 11: 708.3333, 12: 400.0})), Row(features=DenseVector([391.2631, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 392.2127, 2: 1000.0, 10: 1000.0, 11: 875.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 393.1624, 1: 1000.0, 2: 1000.0, 8: 1000.0, 10: 1000.0, 11: 875.0, 12: 400.0})), Row(features=SparseVector(14, {0: 394.1121, 10: 1000.0, 11: 625.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([395.0617, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 396.0114, 1: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 11: 750.0, 12: 500.0})), Row(features=SparseVector(14, {0: 396.9611, 7: 1000.0, 8: 1000.0, 9: 1000.0, 11: 916.6667, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([397.9107, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([398.8604, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 399.8101, 4: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 11: 750.0, 12: 500.0})), Row(features=SparseVector(14, {0: 400.7597, 3: 1000.0, 5: 1000.0, 8: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 401.7094, 4: 1000.0, 5: 1000.0, 8: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([402.6591, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 125.0, 700.0, 0.0])), Row(features=DenseVector([403.6087, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 250.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 404.5584, 1: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 405.5081, 1: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 406.4577, 3: 1000.0, 4: 1000.0, 7: 1000.0, 9: 1000.0, 11: 583.3333, 12: 400.0})), Row(features=SparseVector(14, {0: 407.4074, 7: 1000.0, 10: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([408.3571, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 409.3067, 2: 1000.0, 7: 1000.0, 11: 958.3333, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([410.2564, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=DenseVector([411.2061, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 412.1557, 1: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 413.1054, 1: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 414.0551, 1: 1000.0, 2: 1000.0, 6: 1000.0, 9: 1000.0, 11: 750.0, 12: 400.0})), Row(features=SparseVector(14, {0: 415.0047, 3: 1000.0, 4: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 541.6667, 12: 500.0})), Row(features=DenseVector([415.9544, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 83.3333, 600.0, 0.0])), Row(features=DenseVector([416.9041, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 83.3333, 700.0, 0.0])), Row(features=DenseVector([417.8538, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 583.3333, 700.0, 0.0])), Row(features=DenseVector([418.8034, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([419.7531, 1000.0, 0.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 958.3333, 600.0, 0.0])), Row(features=SparseVector(14, {0: 420.7028, 1: 1000.0, 2: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 11: 666.6667, 12: 500.0})), Row(features=DenseVector([421.6524, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 0.0, 666.6667, 600.0, 0.0])), Row(features=SparseVector(14, {0: 422.6021, 1: 1000.0, 5: 1000.0, 7: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 423.5518, 11: 1000.0, 13: 1000.0})), Row(features=DenseVector([424.5014, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0])), Row(features=DenseVector([425.4511, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=DenseVector([426.4008, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 291.6667, 700.0, 0.0])), Row(features=DenseVector([427.3504, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 428.3001, 1: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 750.0, 12: 400.0})), Row(features=DenseVector([429.2498, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 833.3333, 700.0, 0.0])), Row(features=DenseVector([430.1994, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 958.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 431.1491, 11: 958.3333, 13: 1000.0})), Row(features=DenseVector([432.0988, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 708.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 433.0484, 1: 1000.0, 3: 1000.0, 6: 1000.0, 8: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 433.9981, 1: 1000.0, 6: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 434.9478, 1: 1000.0, 6: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 435.8974, 4: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 125.0, 12: 400.0})), Row(features=SparseVector(14, {0: 436.8471, 9: 1000.0, 11: 125.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 437.7968, 2: 1000.0, 7: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([438.7464, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 291.6667, 800.0, 0.0])), Row(features=DenseVector([439.6961, 0.0, 0.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 600.0, 0.0])), Row(features=DenseVector([440.6458, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([441.5954, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 41.6667, 700.0, 0.0])), Row(features=SparseVector(14, {0: 442.5451, 7: 1000.0, 10: 1000.0, 11: 541.6667, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([443.4948, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 41.6667, 1000.0, 0.0])), Row(features=DenseVector([444.4444, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 445.3941, 1: 1000.0, 2: 1000.0, 4: 1000.0, 7: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([446.3438, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 447.2934, 1: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 448.2431, 1: 1000.0, 3: 1000.0, 6: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 449.1928, 1: 1000.0, 3: 1000.0, 4: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 450.1425, 3: 1000.0, 4: 1000.0, 6: 1000.0, 9: 1000.0, 11: 208.3333, 12: 400.0})), Row(features=SparseVector(14, {0: 451.0921, 3: 1000.0, 4: 1000.0, 11: 208.3333, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([452.0418, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 416.6667, 900.0, 0.0])), Row(features=SparseVector(14, {0: 452.9915, 3: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 8: 1000.0, 11: 333.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 453.9411, 1: 1000.0, 2: 1000.0, 6: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 454.8908, 11: 125.0, 13: 1000.0})), Row(features=DenseVector([455.8405, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 900.0, 0.0])), Row(features=DenseVector([456.7901, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 1000.0, 0.0])), Row(features=DenseVector([457.7398, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 700.0, 0.0])), Row(features=DenseVector([458.6895, 1000.0, 0.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 600.0, 0.0])), Row(features=DenseVector([459.6391, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 666.6667, 900.0, 0.0])), Row(features=DenseVector([460.5888, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 583.3333, 700.0, 0.0])), Row(features=DenseVector([461.5385, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 666.6667, 800.0, 0.0])), Row(features=DenseVector([462.4881, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=DenseVector([463.4378, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 464.3875, 6: 1000.0, 10: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([465.3371, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 1000.0, 0.0])), Row(features=DenseVector([466.2868, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0])), Row(features=DenseVector([467.2365, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 666.6667, 600.0, 0.0])), Row(features=SparseVector(14, {0: 468.1861, 7: 1000.0, 11: 666.6667, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([469.1358, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 500.0, 700.0, 0.0])), Row(features=DenseVector([470.0855, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 916.6667, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 471.0351, 4: 1000.0, 6: 1000.0, 7: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([471.9848, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 833.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 472.9345, 1: 1000.0, 4: 1000.0, 5: 1000.0, 6: 1000.0, 10: 1000.0, 11: 708.3333, 12: 500.0})), Row(features=DenseVector([473.8841, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 625.0, 600.0, 0.0])), Row(features=DenseVector([474.8338, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 541.6667, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 475.7835, 1: 1000.0, 2: 1000.0, 5: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([476.7331, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 800.0, 0.0])), Row(features=DenseVector([477.6828, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 478.6325, 1: 1000.0, 3: 1000.0, 5: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 479.5821, 2: 1000.0, 3: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 480.5318, 2: 1000.0, 4: 1000.0, 6: 1000.0, 9: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 481.4815, 2: 1000.0, 4: 1000.0, 6: 1000.0, 9: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 482.4311, 5: 1000.0, 10: 1000.0, 11: 958.3333, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 483.3808, 5: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 750.0, 12: 500.0})), Row(features=DenseVector([484.3305, 0.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 958.3333, 600.0, 0.0])), Row(features=DenseVector([485.2802, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 791.6667, 800.0, 0.0])), Row(features=SparseVector(14, {0: 486.2298, 1: 1000.0, 10: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 487.1795, 1: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([488.1292, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 489.0788, 1: 1000.0, 3: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 490.0285, 1: 1000.0, 3: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 490.9782, 1: 1000.0, 2: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([491.9278, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 583.3333, 1000.0, 0.0])), Row(features=DenseVector([492.8775, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 416.6667, 800.0, 0.0])), Row(features=SparseVector(14, {0: 493.8272, 1: 1000.0, 2: 1000.0, 7: 1000.0, 10: 1000.0, 11: 500.0, 12: 400.0})), Row(features=SparseVector(14, {0: 494.7768, 3: 1000.0, 4: 1000.0, 5: 1000.0, 8: 1000.0, 9: 1000.0, 11: 291.6667, 12: 500.0})), Row(features=DenseVector([495.7265, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 800.0, 0.0])), Row(features=DenseVector([496.6762, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 375.0, 800.0, 0.0])), Row(features=DenseVector([497.6258, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 333.3333, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 498.5755, 4: 1000.0, 5: 1000.0, 6: 1000.0, 8: 1000.0, 9: 1000.0, 11: 750.0, 12: 500.0})), Row(features=SparseVector(14, {0: 499.5252, 5: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 41.6667, 12: 500.0})), Row(features=DenseVector([500.4748, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 791.6667, 800.0, 0.0])), Row(features=DenseVector([501.4245, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 791.6667, 700.0, 0.0])), Row(features=DenseVector([502.3742, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 208.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 503.3238, 10: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([504.2735, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 505.2232, 6: 1000.0, 8: 1000.0, 10: 1000.0, 11: 41.6667, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([506.1728, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 41.6667, 900.0, 0.0])), Row(features=DenseVector([507.1225, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 41.6667, 900.0, 0.0])), Row(features=SparseVector(14, {0: 508.0722, 8: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 509.0218, 1: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 509.9715, 1: 1000.0, 2: 1000.0, 4: 1000.0, 5: 1000.0, 6: 1000.0, 11: 916.6667, 12: 500.0})), Row(features=DenseVector([510.9212, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 916.6667, 700.0, 0.0])), Row(features=SparseVector(14, {0: 511.8708, 6: 1000.0, 11: 916.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 512.8205, 1: 1000.0, 2: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 513.7702, 1: 1000.0, 2: 1000.0, 5: 1000.0, 7: 1000.0, 10: 1000.0, 11: 541.6667, 12: 500.0})), Row(features=SparseVector(14, {0: 514.7198, 1: 1000.0, 2: 1000.0, 7: 1000.0, 10: 1000.0, 11: 541.6667, 12: 400.0})), Row(features=DenseVector([515.6695, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 1000.0, 0.0])), Row(features=DenseVector([516.6192, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 517.5689, 10: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([518.5185, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 958.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 519.4682, 5: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 958.3333, 12: 500.0})), Row(features=DenseVector([520.4179, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 291.6667, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 521.3675, 2: 1000.0, 3: 1000.0, 4: 1000.0, 7: 1000.0, 9: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 522.3172, 1: 1000.0, 2: 1000.0, 6: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 523.2669, 3: 1000.0, 6: 1000.0, 10: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([524.2165, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=DenseVector([525.1662, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 526.1159, 2: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([527.0655, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 875.0, 800.0, 0.0])), Row(features=DenseVector([528.0152, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 416.6667, 600.0, 0.0])), Row(features=DenseVector([528.9649, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 416.6667, 800.0, 0.0])), Row(features=SparseVector(14, {0: 529.9145, 6: 1000.0, 7: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([530.8642, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 791.6667, 1000.0, 0.0])), Row(features=DenseVector([531.8139, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 708.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 532.7635, 1: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([533.7132, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 534.6629, 8: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 535.6125, 1: 1000.0, 8: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([536.5622, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0])), Row(features=DenseVector([537.5119, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 458.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 538.4615, 1: 1000.0, 2: 1000.0, 5: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([539.4112, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 1000.0, 0.0])), Row(features=DenseVector([540.3609, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 958.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 541.3105, 1: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 542.2602, 1: 1000.0, 7: 1000.0, 10: 1000.0, 11: 166.6667, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 543.2099, 2: 1000.0, 8: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 544.1595, 3: 1000.0, 4: 1000.0, 6: 1000.0, 8: 1000.0, 11: 833.3333, 12: 400.0})), Row(features=DenseVector([545.1092, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 546.0589, 5: 1000.0, 6: 1000.0, 9: 1000.0, 10: 1000.0, 11: 958.3333, 12: 400.0})), Row(features=SparseVector(14, {0: 547.0085, 1: 1000.0, 3: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 11: 500.0, 12: 500.0})), Row(features=DenseVector([547.9582, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 0.0, 500.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 548.9079, 1: 1000.0, 3: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 11: 500.0, 12: 500.0})), Row(features=DenseVector([549.8575, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 750.0, 800.0, 0.0])), Row(features=DenseVector([550.8072, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 583.3333, 600.0, 0.0])), Row(features=DenseVector([551.7569, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 552.7066, 4: 1000.0, 7: 1000.0, 10: 1000.0, 11: 666.6667, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 553.6562, 10: 1000.0, 11: 791.6667, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([554.6059, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 555.5556, 1: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 458.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 556.5052, 1: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([557.4549, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 916.6667, 800.0, 0.0])), Row(features=DenseVector([558.4046, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=DenseVector([559.3542, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=DenseVector([560.3039, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 561.2536, 1: 1000.0, 2: 1000.0, 6: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 562.2032, 1: 1000.0, 6: 1000.0, 8: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 563.1529, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([564.1026, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 565.0522, 8: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 566.0019, 6: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([566.9516, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([567.9012, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 900.0, 0.0])), Row(features=DenseVector([568.8509, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 583.3333, 600.0, 0.0])), Row(features=DenseVector([569.8006, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 570.7502, 5: 1000.0, 7: 1000.0, 8: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 571.6999, 4: 1000.0, 7: 1000.0, 8: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([572.6496, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 625.0, 1000.0, 0.0])), Row(features=DenseVector([573.5992, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0])), Row(features=DenseVector([574.5489, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 575.4986, 4: 1000.0, 7: 1000.0, 9: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 576.4482, 2: 1000.0, 4: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 577.3979, 2: 1000.0, 4: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([578.3476, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0])), Row(features=DenseVector([579.2972, 0.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 750.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 580.2469, 2: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 583.3333, 12: 400.0})), Row(features=DenseVector([581.1966, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 541.6667, 800.0, 0.0])), Row(features=DenseVector([582.1462, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 333.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 583.0959, 3: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 8: 1000.0, 11: 333.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 584.0456, 5: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 11: 333.3333, 12: 500.0})), Row(features=DenseVector([584.9953, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 900.0, 0.0])), Row(features=DenseVector([585.9449, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 800.0, 0.0])), Row(features=DenseVector([586.8946, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 666.6667, 800.0, 0.0])), Row(features=DenseVector([587.8443, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 900.0, 0.0])), Row(features=DenseVector([588.7939, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 1000.0, 0.0])), Row(features=DenseVector([589.7436, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([590.6933, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 708.3333, 900.0, 0.0])), Row(features=DenseVector([591.6429, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 1000.0, 0.0])), Row(features=DenseVector([592.5926, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 625.0, 700.0, 0.0])), Row(features=DenseVector([593.5423, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 416.6667, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 594.4919, 1: 1000.0, 7: 1000.0, 10: 1000.0, 11: 416.6667, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 595.4416, 1: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 416.6667, 12: 400.0})), Row(features=SparseVector(14, {0: 596.3913, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 597.3409, 4: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([598.2906, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 599.2403, 4: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 600.1899, 5: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 916.6667, 12: 500.0})), Row(features=SparseVector(14, {0: 601.1396, 1: 1000.0, 4: 1000.0, 5: 1000.0, 10: 1000.0, 11: 500.0, 12: 400.0})), Row(features=DenseVector([602.0893, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 541.6667, 700.0, 0.0])), Row(features=DenseVector([603.0389, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 541.6667, 600.0, 0.0])), Row(features=SparseVector(14, {0: 603.9886, 1: 1000.0, 10: 1000.0, 11: 750.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 604.9383, 6: 1000.0, 8: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 605.8879, 5: 1000.0, 8: 1000.0, 10: 1000.0, 11: 833.3333, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([606.8376, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 875.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 607.7873, 5: 1000.0, 8: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 608.7369, 10: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([609.6866, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 500.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 610.6363, 4: 1000.0, 6: 1000.0, 7: 1000.0, 11: 541.6667, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 611.5859, 3: 1000.0, 4: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 11: 500.0, 12: 500.0})), Row(features=SparseVector(14, {0: 612.5356, 3: 1000.0, 4: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 11: 500.0, 12: 500.0})), Row(features=DenseVector([613.4853, 0.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 708.3333, 600.0, 0.0])), Row(features=DenseVector([614.4349, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 615.3846, 3: 1000.0, 4: 1000.0, 10: 1000.0, 11: 500.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 616.3343, 3: 1000.0, 4: 1000.0, 8: 1000.0, 10: 1000.0, 11: 500.0, 12: 400.0})), Row(features=SparseVector(14, {0: 617.284, 3: 1000.0, 4: 1000.0, 6: 1000.0, 10: 1000.0, 11: 500.0, 12: 400.0})), Row(features=DenseVector([618.2336, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 0.0, 166.6667, 600.0, 0.0])), Row(features=SparseVector(14, {0: 619.1833, 10: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([620.133, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 0.0, 416.6667, 600.0, 0.0])), Row(features=DenseVector([621.0826, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 416.6667, 600.0, 0.0])), Row(features=DenseVector([622.0323, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 833.3333, 600.0, 0.0])), Row(features=DenseVector([622.982, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 250.0, 800.0, 0.0])), Row(features=DenseVector([623.9316, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 1000.0, 0.0])), Row(features=DenseVector([624.8813, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 916.6667, 700.0, 0.0])), Row(features=DenseVector([625.831, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 500.0, 700.0, 0.0])), Row(features=DenseVector([626.7806, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 416.6667, 900.0, 0.0])), Row(features=SparseVector(14, {0: 627.7303, 10: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 628.68, 1: 1000.0, 4: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 629.6296, 1: 1000.0, 5: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 11: 83.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 630.5793, 6: 1000.0, 10: 1000.0, 11: 750.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([631.529, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 632.4786, 11: 1000.0, 13: 1000.0})), Row(features=DenseVector([633.4283, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 0.0, 500.0, 700.0, 0.0])), Row(features=DenseVector([634.378, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 83.3333, 800.0, 0.0])), Row(features=DenseVector([635.3276, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 83.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 636.2773, 1: 1000.0, 2: 1000.0, 7: 1000.0, 10: 1000.0, 11: 583.3333, 12: 400.0})), Row(features=SparseVector(14, {0: 637.227, 1: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 9: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([638.1766, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 0.0, 500.0, 700.0, 0.0])), Row(features=DenseVector([639.1263, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 875.0, 900.0, 0.0])), Row(features=DenseVector([640.076, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 875.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 641.0256, 8: 1000.0, 11: 500.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([641.9753, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 833.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 642.925, 11: 41.6667, 13: 1000.0})), Row(features=DenseVector([643.8746, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 791.6667, 600.0, 0.0])), Row(features=SparseVector(14, {0: 644.8243, 1: 1000.0, 3: 1000.0, 4: 1000.0, 6: 1000.0, 8: 1000.0, 11: 208.3333, 12: 500.0})), Row(features=DenseVector([645.774, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 291.6667, 900.0, 0.0])), Row(features=DenseVector([646.7236, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 291.6667, 900.0, 0.0])), Row(features=SparseVector(14, {0: 647.6733, 2: 1000.0, 5: 1000.0, 6: 1000.0, 8: 1000.0, 10: 1000.0, 11: 708.3333, 12: 500.0})), Row(features=DenseVector([648.623, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([649.5726, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 83.3333, 900.0, 0.0])), Row(features=DenseVector([650.5223, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 166.6667, 800.0, 0.0])), Row(features=DenseVector([651.472, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 83.3333, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 652.4217, 7: 1000.0, 11: 750.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([653.3713, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 500.0, 600.0, 0.0])), Row(features=DenseVector([654.321, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 900.0, 0.0])), Row(features=DenseVector([655.2707, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 833.3333, 600.0, 0.0])), Row(features=SparseVector(14, {0: 656.2203, 6: 1000.0, 7: 1000.0, 11: 375.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 657.17, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([658.1197, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 625.0, 900.0, 0.0])), Row(features=DenseVector([659.0693, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 875.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 660.019, 2: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 660.9687, 2: 1000.0, 5: 1000.0, 10: 1000.0, 11: 708.3333, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([661.9183, 1000.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 458.3333, 600.0, 0.0])), Row(features=DenseVector([662.868, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 916.6667, 700.0, 0.0])), Row(features=DenseVector([663.8177, 1000.0, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 166.6667, 700.0, 0.0])), Row(features=SparseVector(14, {0: 664.7673, 3: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 665.717, 9: 1000.0, 10: 1000.0, 11: 583.3333, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 666.6667, 2: 1000.0, 3: 1000.0, 10: 1000.0, 11: 958.3333, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([667.6163, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 668.566, 13: 1000.0})), Row(features=SparseVector(14, {0: 669.5157, 6: 1000.0, 8: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 670.4653, 1: 1000.0, 4: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 500.0, 12: 500.0})), Row(features=DenseVector([671.415, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 958.3333, 700.0, 0.0])), Row(features=DenseVector([672.3647, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 125.0, 800.0, 0.0])), Row(features=DenseVector([673.3143, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 125.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 674.264, 1: 1000.0, 4: 1000.0, 7: 1000.0, 11: 625.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([675.2137, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 800.0, 0.0])), Row(features=DenseVector([676.1633, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 416.6667, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 677.113, 1: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 678.0627, 6: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 679.0123, 10: 1000.0, 11: 125.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 679.962, 2: 1000.0, 4: 1000.0, 6: 1000.0, 7: 1000.0, 11: 750.0, 12: 400.0})), Row(features=DenseVector([680.9117, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 681.8613, 1: 1000.0, 2: 1000.0, 11: 208.3333, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 682.811, 1: 1000.0, 2: 1000.0, 4: 1000.0, 11: 208.3333, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 683.7607, 1: 1000.0, 2: 1000.0, 11: 208.3333, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([684.7104, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 708.3333, 700.0, 0.0])), Row(features=DenseVector([685.66, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 583.3333, 1000.0, 0.0])), Row(features=DenseVector([686.6097, 1000.0, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 458.3333, 600.0, 0.0])), Row(features=SparseVector(14, {0: 687.5594, 7: 1000.0, 10: 1000.0, 11: 458.3333, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 688.509, 8: 1000.0, 10: 1000.0, 11: 458.3333, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 689.4587, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([690.4084, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 125.0, 1000.0, 0.0])), Row(features=DenseVector([691.358, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 541.6667, 800.0, 0.0])), Row(features=DenseVector([692.3077, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 875.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 693.2574, 5: 1000.0, 9: 1000.0, 10: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([694.207, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 700.0, 0.0])), Row(features=DenseVector([695.1567, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 666.6667, 600.0, 0.0])), Row(features=DenseVector([696.1064, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 697.056, 6: 1000.0, 10: 1000.0, 11: 791.6667, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([698.0057, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 750.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 698.9554, 1: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 10: 1000.0, 11: 750.0, 12: 500.0})), Row(features=SparseVector(14, {0: 699.905, 1: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([700.8547, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 701.8044, 9: 1000.0, 10: 1000.0, 11: 750.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([702.754, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 708.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 703.7037, 10: 1000.0, 11: 416.6667, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([704.6534, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 916.6667, 600.0, 0.0])), Row(features=DenseVector([705.603, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([706.5527, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 791.6667, 900.0, 0.0])), Row(features=DenseVector([707.5024, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 708.452, 10: 1000.0, 11: 416.6667, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([709.4017, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([710.3514, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 711.301, 1: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 583.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 712.2507, 5: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([713.2004, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([714.15, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 666.6667, 700.0, 0.0])), Row(features=SparseVector(14, {0: 715.0997, 11: 1000.0, 13: 1000.0})), Row(features=DenseVector([716.0494, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([716.9991, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 717.9487, 1: 1000.0, 7: 1000.0, 8: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 718.8984, 3: 1000.0, 4: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 791.6667, 12: 500.0})), Row(features=SparseVector(14, {0: 719.8481, 3: 1000.0, 4: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 791.6667, 12: 500.0})), Row(features=DenseVector([720.7977, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 416.6667, 800.0, 0.0])), Row(features=SparseVector(14, {0: 721.7474, 4: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([722.6971, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([723.6467, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 125.0, 1000.0, 0.0])), Row(features=DenseVector([724.5964, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 916.6667, 600.0, 0.0])), Row(features=DenseVector([725.5461, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 83.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 726.4957, 1: 1000.0, 3: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 11: 625.0, 12: 500.0})), Row(features=DenseVector([727.4454, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 900.0, 0.0])), Row(features=DenseVector([728.3951, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 83.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 729.3447, 7: 1000.0, 11: 125.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 730.2944, 3: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 11: 791.6667, 12: 400.0})), Row(features=SparseVector(14, {0: 731.2441, 11: 208.3333, 13: 1000.0})), Row(features=DenseVector([732.1937, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 666.6667, 900.0, 0.0])), Row(features=DenseVector([733.1434, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 734.0931, 1: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 583.3333, 12: 400.0})), Row(features=DenseVector([735.0427, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 125.0, 700.0, 0.0])), Row(features=DenseVector([735.9924, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 736.9421, 2: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 166.6667, 12: 400.0})), Row(features=DenseVector([737.8917, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 900.0, 0.0])), Row(features=DenseVector([738.8414, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 916.6667, 600.0, 0.0])), Row(features=DenseVector([739.7911, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 500.0, 900.0, 0.0])), Row(features=DenseVector([740.7407, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 416.6667, 700.0, 0.0])), Row(features=SparseVector(14, {0: 741.6904, 3: 1000.0, 4: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([742.6401, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 708.3333, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 743.5897, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 744.5394, 7: 1000.0, 10: 1000.0, 11: 416.6667, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([745.4891, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 83.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 746.4387, 1: 1000.0, 11: 916.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 747.3884, 4: 1000.0, 5: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 748.3381, 1: 1000.0, 2: 1000.0, 3: 1000.0, 5: 1000.0, 7: 1000.0, 11: 958.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 749.2877, 1: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 958.3333, 12: 400.0})), Row(features=SparseVector(14, {0: 750.2374, 1: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 791.6667, 12: 400.0})), Row(features=DenseVector([751.1871, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 752.1368, 1: 1000.0, 6: 1000.0, 7: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([753.0864, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 125.0, 900.0, 0.0])), Row(features=DenseVector([754.0361, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([754.9858, 0.0, 0.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=DenseVector([755.9354, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 756.8851, 1: 1000.0, 7: 1000.0, 10: 1000.0, 11: 625.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 757.8348, 5: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([758.7844, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 900.0, 0.0])), Row(features=DenseVector([759.7341, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 0.0, 500.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 760.6838, 1: 1000.0, 4: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 458.3333, 12: 500.0})), Row(features=DenseVector([761.6334, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 916.6667, 600.0, 0.0])), Row(features=DenseVector([762.5831, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 41.6667, 900.0, 0.0])), Row(features=SparseVector(14, {0: 763.5328, 11: 1000.0, 13: 1000.0})), Row(features=DenseVector([764.4824, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([765.4321, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([766.3818, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([767.3314, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 166.6667, 900.0, 0.0])), Row(features=DenseVector([768.2811, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 500.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 769.2308, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([770.1804, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 0.0, 0.0, 1000.0, 333.3333, 600.0, 0.0])), Row(features=DenseVector([771.1301, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 666.6667, 800.0, 0.0])), Row(features=DenseVector([772.0798, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 708.3333, 900.0, 0.0])), Row(features=SparseVector(14, {0: 773.0294, 11: 541.6667, 13: 1000.0})), Row(features=SparseVector(14, {0: 773.9791, 7: 1000.0, 9: 1000.0, 11: 500.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 774.9288, 1: 1000.0, 5: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 11: 500.0, 12: 500.0})), Row(features=SparseVector(14, {0: 775.8784, 1: 1000.0, 2: 1000.0, 6: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([776.8281, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 291.6667, 900.0, 0.0])), Row(features=DenseVector([777.7778, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 916.6667, 700.0, 0.0])), Row(features=SparseVector(14, {0: 778.7274, 4: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 11: 250.0, 12: 500.0})), Row(features=SparseVector(14, {0: 779.6771, 3: 1000.0, 4: 1000.0, 9: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 780.6268, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 500.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 781.5764, 1: 1000.0, 2: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 782.5261, 8: 1000.0, 11: 750.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 783.4758, 10: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 784.4255, 1: 1000.0, 7: 1000.0, 11: 541.6667, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 785.3751, 7: 1000.0, 10: 1000.0, 11: 83.3333, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([786.3248, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 83.3333, 1000.0, 0.0])), Row(features=DenseVector([787.2745, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 83.3333, 600.0, 0.0])), Row(features=SparseVector(14, {0: 788.2241, 3: 1000.0, 4: 1000.0, 5: 1000.0, 8: 1000.0, 11: 958.3333, 12: 400.0})), Row(features=DenseVector([789.1738, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 958.3333, 1000.0, 0.0])), Row(features=DenseVector([790.1235, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 291.6667, 900.0, 0.0])), Row(features=SparseVector(14, {0: 791.0731, 4: 1000.0, 11: 791.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 792.0228, 4: 1000.0, 11: 791.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 792.9725, 4: 1000.0, 11: 791.6667, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([793.9221, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 708.3333, 1000.0, 0.0])), Row(features=DenseVector([794.8718, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 795.8215, 11: 666.6667, 13: 1000.0})), Row(features=SparseVector(14, {0: 796.7711, 1: 1000.0, 3: 1000.0, 4: 1000.0, 6: 1000.0, 7: 1000.0, 11: 583.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 797.7208, 1: 1000.0, 2: 1000.0, 7: 1000.0, 8: 1000.0, 11: 666.6667, 12: 400.0})), Row(features=SparseVector(14, {0: 798.6705, 2: 1000.0, 5: 1000.0, 11: 833.3333, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([799.6201, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 800.5698, 2: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([801.5195, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 250.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 802.4691, 7: 1000.0, 10: 1000.0, 11: 375.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 803.4188, 7: 1000.0, 10: 1000.0, 11: 375.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 804.3685, 1: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 375.0, 12: 400.0})), Row(features=DenseVector([805.3181, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 833.3333, 900.0, 0.0])), Row(features=SparseVector(14, {0: 806.2678, 6: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 807.2175, 11: 333.3333, 13: 1000.0})), Row(features=DenseVector([808.1671, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 625.0, 700.0, 0.0])), Row(features=DenseVector([809.1168, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 250.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 810.0665, 4: 1000.0, 5: 1000.0, 6: 1000.0, 8: 1000.0, 11: 500.0, 12: 400.0})), Row(features=SparseVector(14, {0: 811.0161, 2: 1000.0, 6: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 811.9658, 11: 1000.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 812.9155, 1: 1000.0, 2: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([813.8651, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 416.6667, 600.0, 0.0])), Row(features=DenseVector([814.8148, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 1000.0, 0.0])), Row(features=DenseVector([815.7645, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 291.6667, 1000.0, 0.0])), Row(features=DenseVector([816.7142, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 166.6667, 1000.0, 0.0])), Row(features=DenseVector([817.6638, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 583.3333, 900.0, 0.0])), Row(features=DenseVector([818.6135, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 500.0, 900.0, 0.0])), Row(features=DenseVector([819.5632, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 750.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 820.5128, 1: 1000.0, 5: 1000.0, 10: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([821.4625, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 583.3333, 700.0, 0.0])), Row(features=DenseVector([822.4122, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 823.3618, 10: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([824.3115, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([825.2612, 0.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 826.2108, 7: 1000.0, 11: 750.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([827.1605, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 375.0, 1000.0, 0.0])), Row(features=DenseVector([828.1102, 1000.0, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 829.0598, 11: 1000.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 830.0095, 3: 1000.0, 4: 1000.0, 8: 1000.0, 9: 1000.0, 11: 791.6667, 12: 400.0})), Row(features=DenseVector([830.9592, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 750.0, 700.0, 0.0])), Row(features=DenseVector([831.9088, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 832.8585, 4: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 10: 1000.0, 11: 875.0, 12: 500.0})), Row(features=DenseVector([833.8082, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 583.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 834.7578, 1: 1000.0, 10: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 835.7075, 1: 1000.0, 2: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 836.6572, 1: 1000.0, 2: 1000.0, 7: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([837.6068, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 838.5565, 4: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 11: 500.0, 12: 500.0})), Row(features=SparseVector(14, {0: 839.5062, 11: 1000.0, 13: 1000.0})), Row(features=DenseVector([840.4558, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 841.4055, 3: 1000.0, 4: 1000.0, 5: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 842.3552, 2: 1000.0, 3: 1000.0, 5: 1000.0, 7: 1000.0, 9: 1000.0, 11: 916.6667, 12: 500.0})), Row(features=SparseVector(14, {0: 843.3048, 2: 1000.0, 3: 1000.0, 5: 1000.0, 11: 916.6667, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 844.2545, 2: 1000.0, 3: 1000.0, 5: 1000.0, 11: 916.6667, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([845.2042, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 846.1538, 11: 500.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 847.1035, 4: 1000.0, 5: 1000.0, 6: 1000.0, 8: 1000.0, 10: 1000.0, 11: 708.3333, 12: 500.0})), Row(features=DenseVector([848.0532, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 333.3333, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 849.0028, 5: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 750.0, 12: 500.0})), Row(features=SparseVector(14, {0: 849.9525, 11: 583.3333, 13: 1000.0})), Row(features=DenseVector([850.9022, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([851.8519, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 1000.0, 0.0])), Row(features=DenseVector([852.8015, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 625.0, 900.0, 0.0])), Row(features=DenseVector([853.7512, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 458.3333, 900.0, 0.0])), Row(features=DenseVector([854.7009, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 458.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 855.6505, 3: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 9: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([856.6002, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 416.6667, 800.0, 0.0])), Row(features=DenseVector([857.5499, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([858.4995, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 600.0, 0.0])), Row(features=DenseVector([859.4492, 0.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 583.3333, 600.0, 0.0])), Row(features=SparseVector(14, {0: 860.3989, 11: 666.6667, 13: 1000.0})), Row(features=DenseVector([861.3485, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 375.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 862.2982, 1: 1000.0, 2: 1000.0, 7: 1000.0, 9: 1000.0, 10: 1000.0, 11: 541.6667, 12: 500.0})), Row(features=DenseVector([863.2479, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 666.6667, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 864.1975, 1: 1000.0, 2: 1000.0, 3: 1000.0, 7: 1000.0, 9: 1000.0, 11: 41.6667, 12: 500.0})), Row(features=DenseVector([865.1472, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 125.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 866.0969, 1: 1000.0, 2: 1000.0, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 458.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 867.0465, 9: 1000.0, 11: 500.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 867.9962, 3: 1000.0, 4: 1000.0, 9: 1000.0, 10: 1000.0, 11: 500.0, 12: 400.0})), Row(features=DenseVector([868.9459, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 869.8955, 7: 1000.0, 10: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([870.8452, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 708.3333, 800.0, 0.0])), Row(features=DenseVector([871.7949, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 872.7445, 5: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 873.6942, 8: 1000.0, 10: 1000.0, 11: 625.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([874.6439, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 833.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 875.5935, 6: 1000.0, 7: 1000.0, 8: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([876.5432, 1000.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 877.4929, 1: 1000.0, 4: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([878.4425, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 879.3922, 1: 1000.0, 3: 1000.0, 4: 1000.0, 9: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 880.3419, 3: 1000.0, 4: 1000.0, 5: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 881.2915, 1: 1000.0, 4: 1000.0, 9: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 882.2412, 3: 1000.0, 4: 1000.0, 5: 1000.0, 9: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 883.1909, 3: 1000.0, 4: 1000.0, 5: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 884.1406, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 333.3333, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 885.0902, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 886.0399, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 886.9896, 4: 1000.0, 8: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([887.9392, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 833.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 888.8889, 5: 1000.0, 6: 1000.0, 8: 1000.0, 10: 1000.0, 11: 958.3333, 12: 400.0})), Row(features=SparseVector(14, {0: 889.8386, 3: 1000.0, 10: 1000.0, 11: 291.6667, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 890.7882, 1: 1000.0, 3: 1000.0, 7: 1000.0, 10: 1000.0, 11: 333.3333, 12: 400.0})), Row(features=SparseVector(14, {0: 891.7379, 1: 1000.0, 3: 1000.0, 10: 1000.0, 11: 333.3333, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 892.6876, 1: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 893.6372, 1: 1000.0, 10: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 894.5869, 13: 1000.0})), Row(features=DenseVector([895.5366, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 896.4862, 1: 1000.0, 2: 1000.0, 6: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([897.4359, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 125.0, 700.0, 0.0])), Row(features=DenseVector([898.3856, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 899.3352, 5: 1000.0, 6: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 900.2849, 7: 1000.0, 10: 1000.0, 11: 500.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 901.2346, 5: 1000.0, 7: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([902.1842, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 625.0, 900.0, 0.0])), Row(features=DenseVector([903.1339, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 625.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 904.0836, 1: 1000.0, 2: 1000.0, 5: 1000.0, 8: 1000.0, 9: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 905.0332, 1: 1000.0, 5: 1000.0, 6: 1000.0, 8: 1000.0, 10: 1000.0, 11: 958.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 905.9829, 1: 1000.0, 5: 1000.0, 11: 250.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([906.9326, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 907.8822, 1: 1000.0, 2: 1000.0, 3: 1000.0, 4: 1000.0, 8: 1000.0, 11: 416.6667, 12: 500.0})), Row(features=DenseVector([908.8319, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 708.3333, 900.0, 0.0])), Row(features=SparseVector(14, {0: 909.7816, 2: 1000.0, 7: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 910.7312, 2: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([911.6809, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 0.0, 625.0, 600.0, 0.0])), Row(features=DenseVector([912.6306, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 583.3333, 600.0, 0.0])), Row(features=DenseVector([913.5802, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 583.3333, 800.0, 0.0])), Row(features=SparseVector(14, {0: 914.5299, 5: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([915.4796, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 166.6667, 700.0, 0.0])), Row(features=DenseVector([916.4292, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 666.6667, 600.0, 0.0])), Row(features=DenseVector([917.3789, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 250.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 918.3286, 1: 1000.0, 7: 1000.0, 9: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 919.2783, 5: 1000.0, 6: 1000.0, 8: 1000.0, 10: 1000.0, 11: 958.3333, 12: 400.0})), Row(features=SparseVector(14, {0: 920.2279, 1: 1000.0, 7: 1000.0, 10: 1000.0, 11: 750.0, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 921.1776, 1: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 750.0, 12: 400.0})), Row(features=SparseVector(14, {0: 922.1273, 1: 1000.0, 2: 1000.0, 6: 1000.0, 8: 1000.0, 10: 1000.0, 11: 875.0, 12: 500.0})), Row(features=SparseVector(14, {0: 923.0769, 4: 1000.0, 11: 500.0, 12: 100.0, 13: 1000.0})), Row(features=DenseVector([924.0266, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 924.9763, 1: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=SparseVector(14, {0: 925.9259, 1: 1000.0, 2: 1000.0, 3: 1000.0, 7: 1000.0, 8: 1000.0, 11: 500.0, 12: 500.0})), Row(features=SparseVector(14, {0: 926.8756, 1: 1000.0, 2: 1000.0, 4: 1000.0, 10: 1000.0, 11: 166.6667, 12: 400.0})), Row(features=SparseVector(14, {0: 927.8253, 6: 1000.0, 7: 1000.0, 10: 1000.0, 11: 500.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([928.7749, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 750.0, 700.0, 0.0])), Row(features=DenseVector([929.7246, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 0.0, 1000.0, 700.0, 0.0])), Row(features=DenseVector([930.6743, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 500.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 931.6239, 4: 1000.0, 5: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([932.5736, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([933.5233, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 916.6667, 1000.0, 0.0])), Row(features=DenseVector([934.4729, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 500.0, 600.0, 0.0])), Row(features=DenseVector([935.4226, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 458.3333, 700.0, 0.0])), Row(features=SparseVector(14, {0: 936.3723, 1: 1000.0, 4: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([937.3219, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 750.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 938.2716, 5: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=DenseVector([939.2213, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 958.3333, 900.0, 0.0])), Row(features=DenseVector([940.1709, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 958.3333, 800.0, 0.0])), Row(features=DenseVector([941.1206, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 875.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 942.0703, 1: 1000.0, 4: 1000.0, 7: 1000.0, 10: 1000.0, 11: 916.6667, 12: 400.0})), Row(features=DenseVector([943.0199, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 500.0, 1000.0, 0.0])), Row(features=SparseVector(14, {0: 943.9696, 1: 1000.0, 2: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 944.9193, 11: 1000.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 945.8689, 2: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 946.8186, 4: 1000.0, 9: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([947.7683, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 875.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 948.7179, 2: 1000.0, 3: 1000.0, 4: 1000.0, 6: 1000.0, 11: 458.3333, 12: 400.0})), Row(features=SparseVector(14, {0: 949.6676, 6: 1000.0, 10: 1000.0, 11: 1000.0, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 950.6173, 1: 1000.0, 5: 1000.0, 6: 1000.0, 8: 1000.0, 10: 1000.0, 11: 916.6667, 12: 500.0})), Row(features=SparseVector(14, {0: 951.567, 5: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 952.5166, 1: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 9: 1000.0, 11: 625.0, 12: 500.0})), Row(features=DenseVector([953.4663, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 800.0, 0.0])), Row(features=DenseVector([954.416, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 0.0, 1000.0, 500.0, 600.0, 0.0])), Row(features=DenseVector([955.3656, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 0.0, 1000.0, 500.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 956.3153, 4: 1000.0, 7: 1000.0, 10: 1000.0, 11: 916.6667, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([957.265, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 833.3333, 900.0, 0.0])), Row(features=SparseVector(14, {0: 958.2146, 1: 1000.0, 3: 1000.0, 4: 1000.0, 7: 1000.0, 10: 1000.0, 11: 500.0, 12: 500.0})), Row(features=DenseVector([959.1643, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 960.114, 2: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 961.0636, 7: 1000.0, 10: 1000.0, 11: 833.3333, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([962.0133, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 750.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 962.963, 1: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 9: 1000.0, 11: 750.0, 12: 500.0})), Row(features=SparseVector(14, {0: 963.9126, 3: 1000.0, 4: 1000.0, 7: 1000.0, 9: 1000.0, 11: 500.0, 12: 400.0})), Row(features=SparseVector(14, {0: 964.8623, 2: 1000.0, 5: 1000.0, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 500.0})), Row(features=DenseVector([965.812, 1000.0, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 966.7616, 3: 1000.0, 4: 1000.0, 9: 1000.0, 11: 375.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([967.7113, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 968.661, 5: 1000.0, 9: 1000.0, 11: 291.6667, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([969.6106, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 0.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 970.5603, 11: 1000.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 971.51, 1: 1000.0, 2: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 400.0})), Row(features=SparseVector(14, {0: 972.4596, 10: 1000.0, 11: 958.3333, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 973.4093, 7: 1000.0, 8: 1000.0, 10: 1000.0, 11: 458.3333, 12: 300.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 974.359, 10: 1000.0, 11: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 975.3086, 3: 1000.0, 4: 1000.0, 5: 1000.0, 8: 1000.0, 9: 1000.0, 11: 708.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 976.2583, 3: 1000.0, 4: 1000.0, 5: 1000.0, 6: 1000.0, 9: 1000.0, 11: 416.6667, 12: 500.0})), Row(features=DenseVector([977.208, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 800.0, 0.0])), Row(features=SparseVector(14, {0: 978.1576, 5: 1000.0, 8: 1000.0, 10: 1000.0, 11: 1000.0, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([979.1073, 1000.0, 1000.0, 0.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 600.0, 0.0])), Row(features=SparseVector(14, {0: 980.057, 1: 1000.0, 3: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 11: 833.3333, 12: 500.0})), Row(features=SparseVector(14, {0: 981.0066, 6: 1000.0, 10: 1000.0, 11: 166.6667, 12: 200.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 981.9563, 8: 1000.0, 11: 666.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 982.906, 1: 1000.0, 4: 1000.0, 6: 1000.0, 7: 1000.0, 8: 1000.0, 11: 250.0, 12: 500.0})), Row(features=DenseVector([983.8557, 1000.0, 1000.0, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 83.3333, 600.0, 0.0])), Row(features=SparseVector(14, {0: 984.8053, 3: 1000.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 985.755, 1: 1000.0, 2: 1000.0, 7: 1000.0, 9: 1000.0, 11: 583.3333, 12: 400.0})), Row(features=SparseVector(14, {0: 986.7047, 5: 1000.0, 6: 1000.0, 7: 1000.0, 9: 1000.0, 11: 583.3333, 12: 400.0})), Row(features=SparseVector(14, {0: 987.6543, 10: 1000.0, 11: 541.6667, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 988.604, 4: 1000.0, 7: 1000.0, 11: 541.6667, 12: 200.0, 13: 1000.0})), Row(features=DenseVector([989.5537, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 800.0, 0.0])), Row(features=DenseVector([990.5033, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 541.6667, 1000.0, 0.0])), Row(features=DenseVector([991.453, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 541.6667, 1000.0, 0.0])), Row(features=DenseVector([992.4027, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 750.0, 1000.0, 0.0])), Row(features=DenseVector([993.3523, 0.0, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 0.0, 500.0, 700.0, 0.0])), Row(features=SparseVector(14, {0: 994.302, 1: 1000.0, 2: 1000.0, 5: 1000.0, 6: 1000.0, 7: 1000.0, 11: 750.0, 12: 500.0})), Row(features=SparseVector(14, {0: 995.2517, 11: 750.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 996.2013, 10: 1000.0, 11: 500.0, 12: 100.0, 13: 1000.0})), Row(features=SparseVector(14, {0: 997.151, 3: 1000.0, 4: 1000.0, 5: 1000.0, 7: 1000.0, 9: 1000.0, 12: 500.0})), Row(features=DenseVector([998.1007, 1000.0, 0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 250.0, 900.0, 0.0])), Row(features=SparseVector(14, {0: 999.0503, 1: 1000.0, 8: 1000.0, 10: 1000.0, 11: 291.6667, 12: 300.0, 13: 1000.0})), Row(features=DenseVector([1000.0, 1000.0, 1000.0, 0.0, 0.0, 1000.0, 1000.0, 0.0, 1000.0, 1000.0, 0.0, 500.0, 600.0, 0.0]))]\n",
            "\u001b[1mModel Weights: \u001b[0m 479\n",
            "Accuracy:  100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example  from Chat GPT **\n"
      ],
      "metadata": {
        "id": "H0u2jpPWlk_V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6-fq_BeKuBU"
      },
      "source": [
        "### Naive Bayes\n",
        "\n",
        "**Recap from the lecture:**\n",
        "The Naive Bayes Classifier is a collection of classification algorithms based on Bayes Theorem. It is not a single algorithm but a family of algorithms that all share a common principle, that every feature being classified is independent of the value of any other feature.\n",
        "\n",
        "So for example, a fruit may be considered to be an apple if it is red, round, and about 3″ in diameter. A Naive Bayes classifier considers each of these “features” (red, round, 3” in diameter) to contribute independently to the probability that the fruit is an apple, regardless of any correlations between features. Features, however, aren’t always independent which is often seen as a shortcoming of the Naive Bayes algorithm and this is why it’s labeled “naive”.\n",
        "\n",
        "**Assumptions:**\n",
        " - Independence between every pair of features\n",
        " - Feature values are nonnegative (which is why we checked earlier)\n",
        "\n",
        "**Hyper Parameters:**\n",
        "\n",
        " - **smoothing** = It is problematic when a frequency-based probability is zero, because it will wipe out all the information in the other probabilities, and we need to find a solution for this. A solution would be Laplace smoothing , which is a technique for smoothing categorical data. In PySpark, this number needs to be be >= 0, default is 1.0'. Also here is a great article that defines smoothing in more detail: https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
        " - **thresholds** = Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. The default value is none.\n",
        " - **weightCol** = If you have a weight column you would enter the name of the column here. If this is not set or empty, we treat all instance weights as 1.0. To learn more about the theory behind this, here is a good paper: http://pami.uwaterloo.ca/~khoury/ece457f07/Zhang2004.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFukxZBSKuBZ",
        "outputId": "88f7948c-cf6f-48f1-f7c2-f04c5702699c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  100.0\n"
          ]
        }
      ],
      "source": [
        "# Add parameters of your choice here:\n",
        "classifier = NaiveBayes()\n",
        "paramGrid = (ParamGridBuilder() \\\n",
        "             .addGrid(classifier.smoothing, [0.0, 0.2, 0.4, 0.6]) \\\n",
        "             .build())\n",
        "\n",
        "#Cross Validator requires all of the following parameters:\n",
        "crossval = CrossValidator(estimator=classifier,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(),\n",
        "                          numFolds=2) # 3 + is best practice\n",
        "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "fitModel = crossval.fit(train)\n",
        "\n",
        "predictions = fitModel.transform(test)\n",
        "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "print(\"Accuracy: \",accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
        "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1, 1.0]).build()\n",
        "\n",
        "crossval = CrossValidator(estimator=lr,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\"),\n",
        "                          numFolds=3)\n",
        "\n",
        "fitModel = crossval.fit(train)\n",
        "predictions = fitModel.transform(test)\n",
        "\n",
        "accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\").evaluate(predictions)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZeOm7yiraZK",
        "outputId": "9d3dae83-bb70-42de-fff3-b62539037161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OXyOKW3ynrW",
        "outputId": "dffadc24-3a53-4197-dfc2-0d37e6c3ec48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- label: double (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZQjKDHuKuBZ"
      },
      "source": [
        "## Linear Support Vector Machine\n",
        "\n",
        "**Recap from lecture:**\n",
        "Linear SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes, which is why you can only use it for binary classification. Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set. Intuitively, the further from the hyperplane our data points lie, the more confident we are that they have been correctly classified. We therefore want our data points to be as far away from the hyperplane as possible, while still being on the correct side of it. So when new testing data is added, whatever side of the hyperplane it lands will decide the class that we assign to it.\n",
        "\n",
        "**Interpretting the coefficients:**\n",
        "\n",
        "Each coefficients direction gives us the predicted class, so if you take the dot product of any point with the vector, you can tell on which side it is: if the dot product is positive, it belongs to the positive class, if it is negative it belongs to the negative class.\n",
        "\n",
        "You can even learn something about the importance of each feature. Let's say the svm would find only one feature useful for separating the data, then the hyperplane would be orthogonal to that axis. So, you could say that the absolute size of the coefficient relative to the other ones gives an indication of how important the feature was for the separation.\n",
        "\n",
        "**Hyper Parameters:** <br>\n",
        "\n",
        "**MaxIter:** <br>\n",
        "The maximum number of iterations to use. There is no clear formula for setting the optimum iteration number, but you can figure out this issue by an iterative process by initializing the iteration number by a small number like 100 and then increase it linearly. This process will be repeated until the MSE of the test does not decrease and even may increase. The below link describes well:\n",
        "https://www.quora.com/What-will-happen-if-I-train-my-neural-networks-with-too-much-iteration\n",
        "\n",
        "**regParam**: <br>\n",
        "The purpose of the regularizer is to encourage simple models and avoid overfitting. To learn more about this concept, here is an interesting article: https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a\n",
        "\n",
        "**PySpark Documentation link:** <br> https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KePNeDXKuBZ",
        "outputId": "e94d7560-680c-4e1b-b1e7-b61685cac919",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: \n",
            "0.07024006303483832\n",
            "\u001b[1m Coefficients\u001b[0m\n",
            "You should compares these relative to eachother\n",
            "Coefficients: \n",
            "[-2.6416684399936473e-05,-0.00021678646389439299,-0.00021247063798584016,-0.00017708427617841435,-0.00020833312770067504,-0.00020326325242308696,-0.00019932766246852227,-0.00020182690355542113,-0.00022190218482960033,-0.00021942051189864165,-0.00018834499900334517,1.6739506946340074e-05,-0.0006227482868813182,0.0017263821559884217]\n",
            "Accuracy:  100.0\n"
          ]
        }
      ],
      "source": [
        "# Count how many classes you have and produce an error if it's more than 2.\n",
        "class_count = final_data.select(countDistinct(\"label\")).collect()\n",
        "classes = class_count[0][0]\n",
        "if classes > 2:\n",
        "    print(\"LinearSVC cannot be used because PySpark currently only accepts binary classification data for this algorithm\")\n",
        "\n",
        "# Add parameters of your choice here:\n",
        "classifier = LinearSVC()\n",
        "paramGrid = (ParamGridBuilder() \\\n",
        "             .addGrid(classifier.maxIter, [10, 15]) \\\n",
        "             .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
        "             .build())\n",
        "\n",
        "#Cross Validator requires all of the following parameters:\n",
        "crossval = CrossValidator(estimator=classifier,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(),\n",
        "                          numFolds=2) # 3 + is best practice\n",
        "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "fitModel = crossval.fit(train)\n",
        "\n",
        "BestModel = fitModel.bestModel\n",
        "\n",
        "print(\"Intercept: \\n\" + str(BestModel.intercept))\n",
        "print('\\033[1m' + \" Coefficients\"+ '\\033[0m')\n",
        "print(\"You should compares these relative to eachother\")\n",
        "print(\"Coefficients: \\n\" + str(BestModel.coefficients))\n",
        "\n",
        "# Automatically gets the best model\n",
        "predictions = fitModel.transform(test)\n",
        "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "print(\"Accuracy: \",accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYJjAA3hKuBZ"
      },
      "source": [
        "## Decision Tree\n",
        "\n",
        "**Recall from the lecture:**\n",
        "Decision Trees classifiers  are a supervised learning method is used to classify a variable by learning from historical data that the model uses to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.\n",
        "\n",
        "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches. Leaf node represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.\n",
        "\n",
        "### Common Hyper Parameters\n",
        "\n",
        " - **maxBins** = Max number of bins for discretizing continuous features. Must be >=2 and >= number of categories for any categorical feature.\n",
        "     - **Continuous features:** For small datasets in single-machine implementations, the split candidates for each continuous feature are typically the unique values for the feature. Some implementations sort the feature values and then use the ordered unique values as split candidates for faster tree calculations.\n",
        "         Sorting feature values is expensive for large distributed datasets. This implementation computes an approximate set of split candidates by performing a quantile calculation over a sampled fraction of the data. The ordered splits create “bins” and the maximum number of such bins can be specified using the maxBins parameter.\n",
        "         Note that the number of bins cannot be greater than the number of instances N (a rare scenario since the default maxBins value is 32). The tree algorithm automatically reduces the number of bins if the condition is not satisfied.\n",
        "\n",
        "     - **Categorical features:** For a categorical feature with M possible values (categories), one could come up with 2 exp(M−1) −1 split candidates. For binary (0/1) classification and regression, we can reduce the number of split candidates to M−1 by ordering the categorical feature values by the average label. For example, for a binary classification problem with one categorical feature with three categories A, B and C whose corresponding proportions of label 1 are 0.2, 0.6 and 0.4, the categorical features are ordered as A, C, B. The two split candidates are A | C, B and A , C | B where | denotes the split.\n",
        "         In multiclass classification, all 2 exp(M−1) −1 possible splits are used whenever possible. When 2 exp(M−1) −1 is greater than the maxBins parameter, we use a (heuristic) method similar to the method used for binary classification and regression. The M categorical feature values are ordered by impurity, and the resulting M−1 split candidates are considered.\n",
        "         \n",
        " - **maxDepth** = The max_depth parameter specifies the maximum depth of each tree. The default value for max_depth is None, which means that each tree will expand until every leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class.\n",
        "\n",
        "### Feature Importance Scores\n",
        "Scores add up to 1 accross all varaibles so the lowest score is the least imporant variable.\n",
        "\n",
        "\n",
        "### Extra Reading\n",
        "**How to tune a decision tree** <br>\n",
        "https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680\n",
        "\n",
        "**PySpark Documentation link:** <br> https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQWGi2aEKuBZ",
        "outputId": "08b29c85-855d-4172-eec9-48d610e293ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Accuracy:  100.0\n"
          ]
        }
      ],
      "source": [
        "# Add parameters of your choice here:\n",
        "classifier = DecisionTreeClassifier()\n",
        "paramGrid = (ParamGridBuilder() \\\n",
        "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
        "             .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
        "             .build())\n",
        "\n",
        "#Cross Validator requires all of the following parameters:\n",
        "crossval = CrossValidator(estimator=classifier,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(),\n",
        "                          numFolds=2) # 3 + is best practice\n",
        "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "fitModel = crossval.fit(train)\n",
        "\n",
        "# Collect and print feature importances\n",
        "BestModel = fitModel.bestModel\n",
        "featureImportances = BestModel.featureImportances.toArray()\n",
        "print(\"Feature Importances: \",featureImportances)\n",
        "\n",
        "predictions = fitModel.transform(test)\n",
        "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "print(\"Accuracy: \",accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOGj3c4GKuBa",
        "outputId": "3aae137e-300e-4f29-95f4-82366950aaaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------+-----+\n",
            "|feature               |score|\n",
            "+----------------------+-----+\n",
            "|Qchat-10-Score        |1    |\n",
            "|A3                    |0    |\n",
            "|A4                    |0    |\n",
            "|Age_Mons              |0    |\n",
            "|Ethnicity             |0    |\n",
            "|A7                    |0    |\n",
            "|A8                    |0    |\n",
            "|Sex                   |0    |\n",
            "|A9                    |0    |\n",
            "|A5                    |0    |\n",
            "|A6                    |0    |\n",
            "|Jaundice              |0    |\n",
            "|Family_mem_with_ASD   |0    |\n",
            "|Who completed the test|0    |\n",
            "|A1                    |0    |\n",
            "|A2                    |0    |\n",
            "|A10                   |0    |\n",
            "+----------------------+-----+\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# zip input_columns qith feature importance scores and create df\n",
        "\n",
        "# First convert featureimportance scores from numpy array to list\n",
        "imp_scores = []\n",
        "for x in featureImportances:\n",
        "    imp_scores.append(int(x))\n",
        "\n",
        "# Then zip with input_columns list and create a df\n",
        "result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
        "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMdXw9x7KuBa"
      },
      "source": [
        "## Random Forest\n",
        "\n",
        "**Recal from the lecture** <br>\n",
        "Suppose you have a training set with 6 classes, random forest may create three decision trees taking input of each subset. Finally, it predicts based on the majority of votes from each of the decision trees made. This works well because a single decision tree may be prone to noise, but aggregate of many decision trees reduce the effect of noise giving more accurate results. The subsets in different decision trees created may overlap.\n",
        "\n",
        "\n",
        "### Common Hyper Parameters\n",
        "\n",
        " - **maxBins** = Max number of bins for discretizing continuous features. Must be >=2 and >= number of categories for any categorical feature.\n",
        "     - **Continuous features:** For small datasets in single-machine implementations, the split candidates for each continuous feature are typically the unique values for the feature. Some implementations sort the feature values and then use the ordered unique values as split candidates for faster tree calculations.\n",
        "         Sorting feature values is expensive for large distributed datasets. This implementation computes an approximate set of split candidates by performing a quantile calculation over a sampled fraction of the data. The ordered splits create “bins” and the maximum number of such bins can be specified using the maxBins parameter.\n",
        "         Note that the number of bins cannot be greater than the number of instances N (a rare scenario since the default maxBins value is 32). The tree algorithm automatically reduces the number of bins if the condition is not satisfied.\n",
        "\n",
        "     - **Categorical features:** For a categorical feature with M possible values (categories), one could come up with 2 exp(M−1) −1 split candidates. For binary (0/1) classification and regression, we can reduce the number of split candidates to M−1 by ordering the categorical feature values by the average label. For example, for a binary classification problem with one categorical feature with three categories A, B and C whose corresponding proportions of label 1 are 0.2, 0.6 and 0.4, the categorical features are ordered as A, C, B. The two split candidates are A | C, B and A , C | B where | denotes the split.\n",
        "         In multiclass classification, all 2 exp(M−1) −1 possible splits are used whenever possible. When 2 exp(M−1) −1 is greater than the maxBins parameter, we use a (heuristic) method similar to the method used for binary classification and regression. The M categorical feature values are ordered by impurity, and the resulting M−1 split candidates are considered.\n",
        "         \n",
        " - **maxDepth** = The maxDepth parameter specifies the maximum depth of each tree. The default value for max_depth is None, which means that each tree will expand until every leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class.\n",
        "\n",
        "### Feature Importance Scores\n",
        "Scores add up to 1 accross all varaibles so the lowest score is the least imporant variable.\n",
        "\n",
        "PySpark Documentation link: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "srnYzq8wKuBa",
        "outputId": "40a1f93b-e626-4d82-d930-c7aea11d5aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-9030c860f431>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Fit Model: Run cross-validation, and choose the best set of parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mfitModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Retrieve best model from cross val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ],
      "source": [
        "# Add parameters of your choice here:\n",
        "classifier = RandomForestClassifier()\n",
        "paramGrid = (ParamGridBuilder() \\\n",
        "               .addGrid(classifier.maxDepth, [2, 5, 10])\n",
        "#                                .addGrid(classifier.maxBins, [5, 10, 20])\n",
        "#                                .addGrid(classifier.numTrees, [5, 20, 50])\n",
        "             .build())\n",
        "\n",
        "#Cross Validator requires all of the following parameters:\n",
        "crossval = CrossValidator(estimator=classifier,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(),\n",
        "                          numFolds=2) # 3 + is best practice\n",
        "\n",
        "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "fitModel = crossval.fit(train)\n",
        "\n",
        "# Retrieve best model from cross val\n",
        "BestModel = fitModel.bestModel\n",
        "featureImportances = BestModel.featureImportances.toArray()\n",
        "print(\"Feature Importances: \",featureImportances)\n",
        "\n",
        "predictions = fitModel.transform(test)\n",
        "\n",
        "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "print(\" \")\n",
        "print(\"Accuracy: \",accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bly1r5ESKuBa"
      },
      "source": [
        "## Gradient Boost Tree Classifier\n",
        "\n",
        "**Recall from the lecture**\n",
        "With gradient boosting, it’s more of a hierarchical approach. It combines the weak learners (binary splits) to strong prediction rules that allow a flexble partition of the feature space. The objective here, as is of any supervised learning algorithm, is to define a loss function and minimize it.\n",
        "\n",
        "### Common Hyper Parameters\n",
        "\n",
        " - **maxBins** = Max number of bins for discretizing continuous features. Must be >=2 and >= number of categories for any categorical feature.\n",
        "     - **Continuous features:** For small datasets in single-machine implementations, the split candidates for each continuous feature are typically the unique values for the feature. Some implementations sort the feature values and then use the ordered unique values as split candidates for faster tree calculations.\n",
        "         Sorting feature values is expensive for large distributed datasets. This implementation computes an approximate set of split candidates by performing a quantile calculation over a sampled fraction of the data. The ordered splits create “bins” and the maximum number of such bins can be specified using the maxBins parameter.\n",
        "         Note that the number of bins cannot be greater than the number of instances N (a rare scenario since the default maxBins value is 32). The tree algorithm automatically reduces the number of bins if the condition is not satisfied.\n",
        "\n",
        "     - **Categorical features:** For a categorical feature with M possible values (categories), one could come up with 2 exp(M−1) −1 split candidates. For binary (0/1) classification and regression, we can reduce the number of split candidates to M−1 by ordering the categorical feature values by the average label. For example, for a binary classification problem with one categorical feature with three categories A, B and C whose corresponding proportions of label 1 are 0.2, 0.6 and 0.4, the categorical features are ordered as A, C, B. The two split candidates are A | C, B and A , C | B where | denotes the split.\n",
        "         In multiclass classification, all 2 exp(M−1) −1 possible splits are used whenever possible. When 2 exp(M−1) −1 is greater than the maxBins parameter, we use a (heuristic) method similar to the method used for binary classification and regression. The M categorical feature values are ordered by impurity, and the resulting M−1 split candidates are considered.\n",
        "         \n",
        " - **maxDepth** = The maxDepth parameter specifies the maximum depth of each tree. The default value for max_depth is None, which means that each tree will expand until every leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class.\n",
        "\n",
        "### Feature Importance Scores\n",
        "Scores add up to 1 accross all varaibles so the lowest score is the least imporant variable.\n",
        "\n",
        "PySpark Documentation link: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.GBTClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lampp4o9KuBa",
        "outputId": "2dc0e8f2-5524-4d23-d834-3d7f120f48a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:  [2.98081488e-16 3.80529559e-18 0.00000000e+00 4.00824469e-16\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 5.24547392e-01 4.75452608e-01]\n",
            " \n",
            "Accuracy:  100.0\n"
          ]
        }
      ],
      "source": [
        "class_count = final_data.select(countDistinct(\"label\")).collect()\n",
        "classes = class_count[0][0]\n",
        "if classes > 2:\n",
        "    print(\"GBTClassifier cannot be used because PySpark currently only accepts binary classification data for this algorithm\")\n",
        "\n",
        "# Add parameters of your choice here:\n",
        "classifier = GBTClassifier()\n",
        "\n",
        "paramGrid = (ParamGridBuilder() \\\n",
        "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
        "#                              .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
        "             .addGrid(classifier.maxIter, [10, 15,50,100])\n",
        "             .build())\n",
        "\n",
        "#Cross Validator requires all of the following parameters:\n",
        "crossval = CrossValidator(estimator=classifier,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(),\n",
        "                          numFolds=2) # 3 + is best practice\n",
        "\n",
        "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "fitModel = crossval.fit(train)\n",
        "\n",
        "BestModel = fitModel.bestModel\n",
        "featureImportances = BestModel.featureImportances.toArray()\n",
        "print(\"Feature Importances: \",featureImportances)\n",
        "\n",
        "predictions = fitModel.transform(test)\n",
        "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "print(\" \")\n",
        "print(\"Accuracy: \",accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFy5zusGKuBa"
      },
      "source": [
        "## That's it!\n",
        "\n",
        "Great job guys!\n",
        "\n",
        "### Next up\n",
        "\n",
        "We will learn how to add functions to this script to make it a bit easier to use."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**simple  example  of prediction  Chat GPT **"
      ],
      "metadata": {
        "id": "6zvLQklSqgHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Setup the Spark Environment"
      ],
      "metadata": {
        "id": "8Sk1VNvLsTqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import Row\n",
        "import random\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"ModelExample\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "8EJafYRaszzQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create a Sample Dataset (Simulating Patient Data)"
      ],
      "metadata": {
        "id": "zkVN4neusOb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data (age, blood_pressure, heart_rate, sick [1 = sick, 0 = not sick])\n",
        "data = [\n",
        "    (25, 120, 80, 0),\n",
        "    (45, 150, 85, 1),\n",
        "    (30, 130, 75, 0),\n",
        "    (60, 160, 90, 1),\n",
        "    (35, 140, 70, 0),\n",
        "    (50, 155, 88, 1),\n",
        "    (40, 145, 82, 0)\n",
        "]\n",
        "\n",
        "# Create a DataFrame\n",
        "columns = ['age', 'blood_pressure', 'heart_rate', 'sick']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the dataset\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWGrG8p2qtjU",
        "outputId": "b9a28dd5-bca8-4d37-cc54-425a8baddc7d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+----------+----+\n",
            "|age|blood_pressure|heart_rate|sick|\n",
            "+---+--------------+----------+----+\n",
            "| 25|           120|        80|   0|\n",
            "| 45|           150|        85|   1|\n",
            "| 30|           130|        75|   0|\n",
            "| 60|           160|        90|   1|\n",
            "| 35|           140|        70|   0|\n",
            "| 50|           155|        88|   1|\n",
            "| 40|           145|        82|   0|\n",
            "+---+--------------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Prepare Data for Training the Mode"
      ],
      "metadata": {
        "id": "I9LP4nHFrM-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s prepare the data for training by combining the features into a single vector (which is the format PySpark requires for input into machine learning models)."
      ],
      "metadata": {
        "id": "euUH9PcDr1pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the feature columns and the target column\n",
        "feature_columns = ['age', 'blood_pressure', 'heart_rate']\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Transform the data to create a feature vector\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Show the transformed data\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85aO-fC3rP_X",
        "outputId": "377ce6da-2d17-45d7-be2a-127558476e3d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+----------+----+-----------------+\n",
            "|age|blood_pressure|heart_rate|sick|         features|\n",
            "+---+--------------+----------+----+-----------------+\n",
            "| 25|           120|        80|   0|[25.0,120.0,80.0]|\n",
            "| 45|           150|        85|   1|[45.0,150.0,85.0]|\n",
            "| 30|           130|        75|   0|[30.0,130.0,75.0]|\n",
            "| 60|           160|        90|   1|[60.0,160.0,90.0]|\n",
            "| 35|           140|        70|   0|[35.0,140.0,70.0]|\n",
            "| 50|           155|        88|   1|[50.0,155.0,88.0]|\n",
            "| 40|           145|        82|   0|[40.0,145.0,82.0]|\n",
            "+---+--------------+----------+----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Train a Logistic Regression Model"
      ],
      "metadata": {
        "id": "3vV-td6GtmDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=1234)\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "lr = LogisticRegression(labelCol=\"sick\", featuresCol=\"features\")\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Show predictions\n",
        "predictions.select(\"age\", \"blood_pressure\", \"heart_rate\", \"prediction\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lekeFWyMtoiv",
        "outputId": "4b79c214-0a27-4ead-effd-17cddbce3ae9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+----------+----------+\n",
            "|age|blood_pressure|heart_rate|prediction|\n",
            "+---+--------------+----------+----------+\n",
            "| 30|           130|        75|       0.0|\n",
            "+---+--------------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5I18SxgyrO0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Test the Model with a New Row of Data (A New Patient)"
      ],
      "metadata": {
        "id": "aUsjqPj6uXT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New patient data (age, blood_pressure, heart_rate)\n",
        "new_patient_data = Row(age=55, blood_pressure=145, heart_rate=85)\n",
        "\n",
        "# Create a DataFrame for the new patient\n",
        "new_patient_df = spark.createDataFrame([new_patient_data])\n",
        "\n",
        "# Transform the new patient's data into the feature vector\n",
        "new_patient_df = assembler.transform(new_patient_df)\n",
        "\n",
        "# Make a prediction for the new patient\n",
        "new_patient_prediction = lr_model.transform(new_patient_df)\n",
        "\n",
        "# Show the prediction (sick or not sick)\n",
        "new_patient_prediction.select(\"age\", \"blood_pressure\", \"heart_rate\", \"prediction\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h13120-ruY-u",
        "outputId": "72aa2db6-26c1-42c0-990f-851f05184433"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+----------+----------+\n",
            "|age|blood_pressure|heart_rate|prediction|\n",
            "+---+--------------+----------+----------+\n",
            "| 55|           145|        85|       1.0|\n",
            "+---+--------------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5oZ-Bj87z3Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_patient_with_features.select(\"features\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuJiqf5j_MYc",
        "outputId": "a3dacb93-be4d-40ce-c648-cf1a7b978745"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+\n",
            "|features                                          |\n",
            "+--------------------------------------------------+\n",
            "|[0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,1.0,0.0,28.0,3.0]|\n",
            "+--------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Chat GPT"
      ],
      "metadata": {
        "id": "tttLdARuBkKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Instantiate the base classifier\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Instantiate the OneVsRest Classifier\n",
        "classifier = OneVsRest(classifier=lr)\n",
        "\n",
        "# Add parameters of your choice here:\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .build()\n",
        "\n",
        "# Cross Validator requires the following parameters\n",
        "crossval = CrossValidator(estimator=classifier,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(),\n",
        "                          numFolds=2)  # 3 is best practice\n",
        "\n",
        "# Run cross-validation and choose the best set of parameters\n",
        "fitModel = crossval.fit(train)\n",
        "\n",
        "# Print the Coefficients\n",
        "BestModel = fitModel.bestModel\n",
        "models = BestModel.models\n",
        "for model in models:\n",
        "    print('\\033[1m' + 'Intercept: ' + '\\033[0m', model.intercept, '\\033[1m' + '\\nCoefficients:' + '\\033[0m', model.coefficients)\n",
        "\n",
        "# Now generate predictions on test dataset\n",
        "predictions = fitModel.transform(test)\n",
        "\n",
        "# And calculate the accuracy score\n",
        "accuracy = (MulticlassClassificationEvaluator().evaluate(predictions)) * 100\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# New patient data (age, blood_pressure, heart_rate)\n",
        "new_patient_data = Row(\n",
        "    A1=0,\n",
        "    A2=1,\n",
        "    A3=0,\n",
        "    A4=1,\n",
        "    A5=0,\n",
        "    A6=1,\n",
        "    A7=0,\n",
        "    A8=1,\n",
        "    A9=1,\n",
        "    A10=0,\n",
        "    Age_Mons=28,\n",
        "    Qchat_10_Score=3,\n",
        "    Sex=\"f\",\n",
        "    Ethnicity=\"middle eastern\",\n",
        "    Jaundice=\"no\",\n",
        "    Family_mem_with_ASD=\"no\",\n",
        "    Who_completed_the_test=\"family member\"\n",
        ")\n",
        "\n",
        "# Convert the row into a DataFrame\n",
        "new_patient_df = spark.createDataFrame([new_patient_data])\n",
        "\n",
        "# Preprocess the categorical columns: Index categorical variables\n",
        "indexer_sex = StringIndexer(inputCol=\"Sex\", outputCol=\"Sex_num\")\n",
        "indexer_ethnicity = StringIndexer(inputCol=\"Ethnicity\", outputCol=\"Ethnicity_num\")\n",
        "\n",
        "# Apply transformations to new patient data\n",
        "new_patient_df = indexer_sex.fit(new_patient_df).transform(new_patient_df)\n",
        "new_patient_df = indexer_ethnicity.fit(new_patient_df).transform(new_patient_df)\n",
        "\n",
        "# List of input columns (ensure these match your model training data, including the indexed columns)\n",
        "input_cols = [\"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\", \"A9\", \"A10\", \"Age_Mons\", \"Qchat_10_Score\", \"Sex_num\", \"Ethnicity_num\"]\n",
        "\n",
        "# Assemble the features\n",
        "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
        "\n",
        "# Transform the new patient DataFrame to include the features column\n",
        "new_patient_with_features = assembler.transform(new_patient_df)\n",
        "\n",
        "# Use the trained model to predict\n",
        "predictions = fitModel.transform(new_patient_with_features)\n",
        "\n",
        "# Show the prediction result\n",
        "predictions.select(\"prediction\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiLerfVIBFt5",
        "outputId": "45d4ea0a-6b17-4ca6-b192-4285e7bb322d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mIntercept: \u001b[0m -1.1980249685411162 \u001b[1m\n",
            "Coefficients:\u001b[0m [0.00014902022575136428,0.0004759852475069171,0.00046981460492512934,0.00042161489282809743,0.00048759813916896184,0.0005255558721592255,0.0005576351965753215,0.0006393775042343804,0.00045330316943260267,0.0005753270514516996,0.000309689471533936,0.0001517219399132898,0.001418343867977338,-0.00197715255655174]\n",
            "\u001b[1mIntercept: \u001b[0m 1.1980249685411162 \u001b[1m\n",
            "Coefficients:\u001b[0m [-0.00014902022575136482,-0.0004759852475069169,-0.00046981460492512907,-0.0004216148928280976,-0.00048759813916896184,-0.0005255558721592254,-0.0005576351965753212,-0.0006393775042343803,-0.0004533031694326028,-0.0005753270514516995,-0.0003096894715339361,-0.00015172193991328913,-0.001418343867977338,0.0019771525565517394]\n",
            "Accuracy: 100.0\n",
            "+----------+\n",
            "|prediction|\n",
            "+----------+\n",
            "|       1.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}